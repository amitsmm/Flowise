{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1UeyJn7fEun9msfy_ZuIi3dcT69UZoHL5",
      "authorship_tag": "ABX9TyO5hf/BrFpkOXvm3f0DvwDZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amitsmm/Flowise/blob/main/location_scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rR7GDeTrxFBw",
        "outputId": "357907f1-51b6-4de4-bb89-75592b1e9940"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googlemaps\n",
            "  Downloading googlemaps-4.10.0.tar.gz (33 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests<3.0,>=2.20.0 in /usr/local/lib/python3.12/dist-packages (from googlemaps) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.20.0->googlemaps) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.20.0->googlemaps) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.20.0->googlemaps) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.20.0->googlemaps) (2025.11.12)\n",
            "Building wheels for collected packages: googlemaps\n",
            "  Building wheel for googlemaps (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googlemaps: filename=googlemaps-4.10.0-py3-none-any.whl size=40714 sha256=6f75a3507f5d9f5d45cf3f4fd7a6ac1f24e0bccdfa15e35482566dc85f671776\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/6a/a7/bbc6f5c200032025ee655deb5e163ce8594fa05e67d973aad6\n",
            "Successfully built googlemaps\n",
            "Installing collected packages: googlemaps\n",
            "Successfully installed googlemaps-4.10.0\n"
          ]
        }
      ],
      "source": [
        "pip install googlemaps\n",
        "# to run before executing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#not to be used - does not have the name validation\n",
        "import googlemaps\n",
        "import time\n",
        "import csv\n",
        "\n",
        "# 1. Setup\n",
        "gmaps = googlemaps.Client(key='AIzaSyDlKfGNRGtlfBKe9X_lktXeM79pYPMiO0E')  # Replace with your key\n",
        "cities = [\n",
        "    \"Bengaluru\", \"Mumbai\", \"New Delhi\", \"Chennai\", \"Kolkata\", \"Pune\",\n",
        "    \"Hyderabad\", \"Gurugram\", \"Asansol\", \"Kadapa\", \"Noida\", \"Ahmedabad\",\n",
        "    \"Bharuch\", \"Coimbatore\", \"Trichy\", \"Anand\", \"Lucknow\", \"Faridabad\",\n",
        "    \"Mysore\", \"Jamshedpur\", \"Muzaffarpur\", \"Dhanbad\", \"Goa\", \"Mangalore\",\n",
        "    \"Nashik\", \"Chandigarh\", \"Mohali\", \"Surat\", \"Nagpur\", \"Raipur\",\n",
        "    \"Vijayawada\", \"Guwahati\", \"Ludhiana\", \"Udaipur\", \"Dehradun\", \"Kanpur\",\n",
        "    \"Bhubaneswar\", \"Hubli\", \"Salem\", \"Pondicherry\", \"Jalandhar\", \"Thrissur\",\n",
        "    \"Vadodara\", \"Erode\", \"Bareilly\", \"Manipal\", \"Nellore\", \"Meerut\",\n",
        "    \"Jammu\", \"Rajkot\", \"Vellore\", \"Ghaziabad\", \"Patna\", \"Patiala\",\n",
        "    \"Tirupur\", \"Guntur\", \"Varanasi\", \"Prayagraj\", \"Ranchi\", \"Aurangabad\",\n",
        "    \"Jaipur\", \"Gorakhpur\", \"Tirupati\", \"Durgapur\", \"Kota\", \"Agra\",\n",
        "    \"Siliguri\", \"Bhopal\", \"Kakinada\", \"Indore\", \"Bilaspur\", \"Pathankot\",\n",
        "    \"Jabalpur\", \"Karnal\", \"Visakhapatnam\", \"Amravati\", \"Kochi\", \"Trivandrum\",\n",
        "    \"Amritsar\", \"Madurai\", \"Abu Dhabi\", \"Riyadh\", \"Dubai\", \"Muscat\",\n",
        "    \"Kuala Lumpur\", \"Bahrain\", \"Sharjah\"\n",
        "]\n",
        "\n",
        "records = []   # will store rows as {\"city\": ..., \"place_id\": ...}\n",
        "\n",
        "# 2. Loop through each city\n",
        "for city in cities:\n",
        "    query = f\"Barbeque Nation in {city}\"\n",
        "    print(f\"Searching for: {query}\")\n",
        "\n",
        "    response = gmaps.places(query=query)\n",
        "\n",
        "    while True:\n",
        "        for result in response.get(\"results\", []):\n",
        "            place_id = result.get(\"place_id\")\n",
        "            if place_id:\n",
        "                records.append({\"city\": city, \"place_id\": place_id})\n",
        "\n",
        "        next_token = response.get(\"next_page_token\")\n",
        "        if next_token:\n",
        "            time.sleep(3)\n",
        "            response = gmaps.places(query=query, page_token=next_token)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "# 3. Save to CSV\n",
        "csv_filename = \"barbeque_nation_place_ids.csv\"\n",
        "\n",
        "with open(csv_filename, \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"city\", \"place_id\"])\n",
        "    for row in records:\n",
        "        writer.writerow([row[\"city\"], row[\"place_id\"]])\n",
        "\n",
        "print(f\"\\nSaved {len(records)} rows to {csv_filename}\")\n"
      ],
      "metadata": {
        "id": "26QQb_aT1wRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use this for getting the names of the placese\n",
        "import googlemaps\n",
        "import time\n",
        "import csv\n",
        "\n",
        "# 1. Setup\n",
        "# REPLACE WITH YOUR ACTUAL API KEY\n",
        "gmaps = googlemaps.Client(key='AIzaSyDlKfGNRGtlfBKe9X_lktXeM79pYPMiO0E')\n",
        "\n",
        "cities = [\n",
        "    \"Bengaluru\", \"Mumbai\", \"New Delhi\", \"Chennai\", \"Kolkata\", \"Pune\",\n",
        "    \"Hyderabad\", \"Gurugram\", \"Asansol\", \"Kadapa\", \"Noida\", \"Ahmedabad\",\n",
        "    \"Bharuch\", \"Coimbatore\", \"Trichy\", \"Anand\", \"Lucknow\", \"Faridabad\",\n",
        "    \"Mysore\", \"Jamshedpur\", \"Muzaffarpur\", \"Dhanbad\", \"Goa\", \"Mangalore\",\n",
        "    \"Nashik\", \"Chandigarh\", \"Mohali\", \"Surat\", \"Nagpur\", \"Raipur\",\n",
        "    \"Vijayawada\", \"Guwahati\", \"Ludhiana\", \"Udaipur\", \"Dehradun\", \"Kanpur\",\n",
        "    \"Bhubaneswar\", \"Hubli\", \"Salem\", \"Pondicherry\", \"Jalandhar\", \"Thrissur\",\n",
        "    \"Vadodara\", \"Erode\", \"Bareilly\", \"Manipal\", \"Nellore\", \"Meerut\",\n",
        "    \"Jammu\", \"Rajkot\", \"Vellore\", \"Ghaziabad\", \"Patna\", \"Patiala\",\n",
        "    \"Tirupur\", \"Guntur\", \"Varanasi\", \"Prayagraj\", \"Ranchi\", \"Aurangabad\",\n",
        "    \"Jaipur\", \"Gorakhpur\", \"Tirupati\", \"Durgapur\", \"Kota\", \"Agra\",\n",
        "    \"Siliguri\", \"Bhopal\", \"Kakinada\", \"Indore\", \"Bilaspur\", \"Pathankot\",\n",
        "    \"Jabalpur\", \"Karnal\", \"Visakhapatnam\", \"Amravati\", \"Kochi\", \"Trivandrum\",\n",
        "    \"Amritsar\", \"Madurai\", \"Abu Dhabi\", \"Riyadh\", \"Dubai\", \"Muscat\",\n",
        "    \"Kuala Lumpur\", \"Bahrain\", \"Sharjah\"\n",
        "]\n",
        "\n",
        "records = []   # will store rows as {\"city\": ..., \"place_id\": ..., \"place_name\": ...}\n",
        "\n",
        "# 2. Loop through each city\n",
        "for city in cities:\n",
        "    query = f\"Barbeque Nation in {city}\"\n",
        "    print(f\"Searching for: {query}\")\n",
        "\n",
        "    try:\n",
        "        response = gmaps.places(query=query)\n",
        "    except Exception as e:\n",
        "        print(f\"Error searching for {city}: {e}\")\n",
        "        continue\n",
        "\n",
        "    while True:\n",
        "        results = response.get(\"results\", [])\n",
        "\n",
        "        for result in results:\n",
        "            place_id = result.get(\"place_id\")\n",
        "            place_name = result.get(\"name\", \"\") # Get the name\n",
        "\n",
        "            # FILTER: Strictly check if \"Barbeque Nation\" is in the name\n",
        "            if \"Barbeque Nation\" not in place_name:\n",
        "                # print(f\"Skipped: {place_name}\") # Uncomment to see what is being skipped\n",
        "                continue\n",
        "\n",
        "            if place_id:\n",
        "                records.append({\n",
        "                    \"city\": city,\n",
        "                    \"place_id\": place_id,\n",
        "                    \"place_name\": place_name\n",
        "                })\n",
        "\n",
        "        # Handle Pagination\n",
        "        next_token = response.get(\"next_page_token\")\n",
        "        if next_token:\n",
        "            # Detailed Search requires a short delay for the token to become valid\n",
        "            time.sleep(3)\n",
        "            try:\n",
        "                response = gmaps.places(query=query, page_token=next_token)\n",
        "            except Exception as e:\n",
        "                print(f\"Error on next page for {city}: {e}\")\n",
        "                break\n",
        "        else:\n",
        "            break\n",
        "\n",
        "# 3. Save to CSV\n",
        "csv_filename = \"barbeque_nation_master_place_ids.csv\"\n",
        "\n",
        "print(f\"Writing data to {csv_filename}...\")\n",
        "\n",
        "with open(csv_filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    # Updated Header\n",
        "    writer.writerow([\"city\", \"place_id\", \"place_name\"])\n",
        "\n",
        "    for row in records:\n",
        "        writer.writerow([row[\"city\"], row[\"place_id\"], row[\"place_name\"]])\n",
        "\n",
        "print(f\"\\nSuccess! Saved {len(records)} validated rows to {csv_filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgT3dDfB6ExR",
        "outputId": "e9cfd48f-6c1a-4895-8fd3-a32f8d87c61a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching for: Barbeque Nation in Bengaluru\n",
            "Searching for: Barbeque Nation in Mumbai\n",
            "Searching for: Barbeque Nation in New Delhi\n",
            "Searching for: Barbeque Nation in Chennai\n",
            "Searching for: Barbeque Nation in Kolkata\n",
            "Searching for: Barbeque Nation in Pune\n",
            "Searching for: Barbeque Nation in Hyderabad\n",
            "Searching for: Barbeque Nation in Gurugram\n",
            "Searching for: Barbeque Nation in Asansol\n",
            "Searching for: Barbeque Nation in Kadapa\n",
            "Searching for: Barbeque Nation in Noida\n",
            "Searching for: Barbeque Nation in Ahmedabad\n",
            "Searching for: Barbeque Nation in Bharuch\n",
            "Searching for: Barbeque Nation in Coimbatore\n",
            "Searching for: Barbeque Nation in Trichy\n",
            "Searching for: Barbeque Nation in Anand\n",
            "Searching for: Barbeque Nation in Lucknow\n",
            "Searching for: Barbeque Nation in Faridabad\n",
            "Searching for: Barbeque Nation in Mysore\n",
            "Searching for: Barbeque Nation in Jamshedpur\n",
            "Searching for: Barbeque Nation in Muzaffarpur\n",
            "Searching for: Barbeque Nation in Dhanbad\n",
            "Searching for: Barbeque Nation in Goa\n",
            "Searching for: Barbeque Nation in Mangalore\n",
            "Searching for: Barbeque Nation in Nashik\n",
            "Searching for: Barbeque Nation in Chandigarh\n",
            "Searching for: Barbeque Nation in Mohali\n",
            "Searching for: Barbeque Nation in Surat\n",
            "Searching for: Barbeque Nation in Nagpur\n",
            "Searching for: Barbeque Nation in Raipur\n",
            "Searching for: Barbeque Nation in Vijayawada\n",
            "Searching for: Barbeque Nation in Guwahati\n",
            "Searching for: Barbeque Nation in Ludhiana\n",
            "Searching for: Barbeque Nation in Udaipur\n",
            "Searching for: Barbeque Nation in Dehradun\n",
            "Searching for: Barbeque Nation in Kanpur\n",
            "Searching for: Barbeque Nation in Bhubaneswar\n",
            "Searching for: Barbeque Nation in Hubli\n",
            "Searching for: Barbeque Nation in Salem\n",
            "Searching for: Barbeque Nation in Pondicherry\n",
            "Searching for: Barbeque Nation in Jalandhar\n",
            "Searching for: Barbeque Nation in Thrissur\n",
            "Searching for: Barbeque Nation in Vadodara\n",
            "Searching for: Barbeque Nation in Erode\n",
            "Searching for: Barbeque Nation in Bareilly\n",
            "Searching for: Barbeque Nation in Manipal\n",
            "Searching for: Barbeque Nation in Nellore\n",
            "Searching for: Barbeque Nation in Meerut\n",
            "Searching for: Barbeque Nation in Jammu\n",
            "Searching for: Barbeque Nation in Rajkot\n",
            "Searching for: Barbeque Nation in Vellore\n",
            "Searching for: Barbeque Nation in Ghaziabad\n",
            "Searching for: Barbeque Nation in Patna\n",
            "Searching for: Barbeque Nation in Patiala\n",
            "Searching for: Barbeque Nation in Tirupur\n",
            "Searching for: Barbeque Nation in Guntur\n",
            "Searching for: Barbeque Nation in Varanasi\n",
            "Searching for: Barbeque Nation in Prayagraj\n",
            "Searching for: Barbeque Nation in Ranchi\n",
            "Searching for: Barbeque Nation in Aurangabad\n",
            "Searching for: Barbeque Nation in Jaipur\n",
            "Searching for: Barbeque Nation in Gorakhpur\n",
            "Searching for: Barbeque Nation in Tirupati\n",
            "Searching for: Barbeque Nation in Durgapur\n",
            "Searching for: Barbeque Nation in Kota\n",
            "Searching for: Barbeque Nation in Agra\n",
            "Searching for: Barbeque Nation in Siliguri\n",
            "Searching for: Barbeque Nation in Bhopal\n",
            "Searching for: Barbeque Nation in Kakinada\n",
            "Searching for: Barbeque Nation in Indore\n",
            "Searching for: Barbeque Nation in Bilaspur\n",
            "Searching for: Barbeque Nation in Pathankot\n",
            "Searching for: Barbeque Nation in Jabalpur\n",
            "Searching for: Barbeque Nation in Karnal\n",
            "Searching for: Barbeque Nation in Visakhapatnam\n",
            "Searching for: Barbeque Nation in Amravati\n",
            "Searching for: Barbeque Nation in Kochi\n",
            "Searching for: Barbeque Nation in Trivandrum\n",
            "Searching for: Barbeque Nation in Amritsar\n",
            "Searching for: Barbeque Nation in Madurai\n",
            "Searching for: Barbeque Nation in Abu Dhabi\n",
            "Searching for: Barbeque Nation in Riyadh\n",
            "Searching for: Barbeque Nation in Dubai\n",
            "Searching for: Barbeque Nation in Muscat\n",
            "Searching for: Barbeque Nation in Kuala Lumpur\n",
            "Searching for: Barbeque Nation in Bahrain\n",
            "Searching for: Barbeque Nation in Sharjah\n",
            "Writing data to barbeque_nation_master_place_ids.csv...\n",
            "\n",
            "Success! Saved 219 validated rows to barbeque_nation_master_place_ids.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5mhu6uoWJ0n",
        "outputId": "0dc2834b-4741-4dd6-e473-108e974fa1f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "import time\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "# ================= CONFIGURATION =================\n",
        "API_KEY = \"AIzaSyDlKfGNRGtlfBKe9X_lktXeM79pYPMiO0E\"  # <--- REPLACE THIS\n",
        "\n",
        "OUTPUT_FILE = \"places_reviews_output.csv\"\n",
        "INPUT_CSV = \"input_place_ids.csv\" # Optional: If this file exists, IDs are read from here\n",
        "\n",
        "# Hardcoded list (used if INPUT_CSV doesn't exist)\n",
        "DEFAULT_PLACE_IDS = [\n",
        "    \"ChIJK7aICxoVrjsRnjZbF0TjvNo\", # Example: High Court, Mumbai\n",
        "    \"ChIJZU8k2vU9rjsRK9gXANkis7k\", # Example: Taj Mahal Palace, Mumbai\n",
        "    # Add more Place IDs here\n",
        "]\n",
        "\n",
        "BASE_URL = \"https://maps.googleapis.com/maps/api/place/details/json\"\n",
        "\n",
        "# ================= LOGIC =================\n",
        "\n",
        "def get_city_from_components(components):\n",
        "    \"\"\"\n",
        "    Extracts city based on logic:\n",
        "    1. Look for 'locality'\n",
        "    2. Fallback to 'administrative_area_level_2'\n",
        "    \"\"\"\n",
        "    if not components:\n",
        "        return \"\"\n",
        "\n",
        "    # First pass: Locality\n",
        "    for comp in components:\n",
        "        if \"locality\" in comp.get(\"types\", []):\n",
        "            return comp.get(\"long_name\", \"\")\n",
        "\n",
        "    # Second pass: Administrative Area Level 2\n",
        "    for comp in components:\n",
        "        if \"administrative_area_level_2\" in comp.get(\"types\", []):\n",
        "            return comp.get(\"long_name\", \"\")\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "def format_date(timestamp):\n",
        "    \"\"\"Converts Unix timestamp to YYYY-MM-DD (UTC).\"\"\"\n",
        "    if not timestamp:\n",
        "        return \"\"\n",
        "    try:\n",
        "        return datetime.datetime.utcfromtimestamp(timestamp).strftime('%Y-%m-%d')\n",
        "    except (ValueError, TypeError):\n",
        "        return \"\"\n",
        "\n",
        "def get_place_details(place_id):\n",
        "    \"\"\"\n",
        "    Calls the API, parses response, extracts specific fields,\n",
        "    and sorts reviews by newest.\n",
        "    \"\"\"\n",
        "    params = {\n",
        "        \"place_id\": place_id,\n",
        "        \"key\": API_KEY,\n",
        "        # We don't specify 'fields' parameter to ensure we get everything\n",
        "        # needed (reviews, components, etc) by default.\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(BASE_URL, params=params)\n",
        "        data = response.json()\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] HTTP Request failed for {place_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "    status = data.get(\"status\")\n",
        "\n",
        "    if status != \"OK\":\n",
        "        print(f\"[API ERROR] Place ID {place_id} returned status: {status}\")\n",
        "        # Return None to indicate failure, logic below will handle empty row\n",
        "        return None\n",
        "\n",
        "    result = data.get(\"result\", {})\n",
        "\n",
        "    # 1. Extract Base Info\n",
        "    name = result.get(\"name\", \"\")\n",
        "    rating = result.get(\"rating\", \"\")\n",
        "    user_ratings_total = result.get(\"user_ratings_total\", \"\")\n",
        "    business_status = result.get(\"business_status\", \"\")\n",
        "\n",
        "    # Extract City\n",
        "    city = get_city_from_components(result.get(\"address_components\", []))\n",
        "\n",
        "    # 2. Extract and Sort Reviews\n",
        "    raw_reviews = result.get(\"reviews\", [])\n",
        "\n",
        "    # Sort by time descending (Newest first)\n",
        "    # We use .get('time', 0) to ensure we don't crash if time is missing\n",
        "    sorted_reviews = sorted(raw_reviews, key=lambda x: x.get(\"time\", 0), reverse=True)\n",
        "\n",
        "    # Take top 5\n",
        "    top_5_reviews = sorted_reviews[:5]\n",
        "\n",
        "    # Prepare the flattened data dictionary\n",
        "    extracted_data = {\n",
        "        \"place_id\": place_id,\n",
        "        \"place_name\": name,\n",
        "        \"city\": city,\n",
        "        \"overall_rating\": rating,\n",
        "        \"total_ratings\": user_ratings_total,\n",
        "        \"business_status\": business_status,\n",
        "        \"reviews\": top_5_reviews\n",
        "    }\n",
        "\n",
        "    return extracted_data\n",
        "\n",
        "def main():\n",
        "    # 1. Determine Source of Place IDs\n",
        "    place_ids = []\n",
        "    if os.path.exists(INPUT_CSV):\n",
        "        print(f\"Reading Place IDs from {INPUT_CSV}...\")\n",
        "        try:\n",
        "            with open(INPUT_CSV, mode='r', encoding='utf-8-sig') as f:\n",
        "                reader = csv.DictReader(f)\n",
        "                # Assumes column name is 'place_id' inside the CSV\n",
        "                # If strict single column without header, change logic here.\n",
        "                # Here we try to find a column named 'place_id', or just take the first column\n",
        "                for row in reader:\n",
        "                    if 'place_id' in row:\n",
        "                        place_ids.append(row['place_id'].strip())\n",
        "                    else:\n",
        "                        # Fallback: values() returns a list of the row's values\n",
        "                        place_ids.append(list(row.values())[0].strip())\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading input CSV: {e}. Using hardcoded list.\")\n",
        "            place_ids = DEFAULT_PLACE_IDS\n",
        "    else:\n",
        "        print(\"No input CSV found. Using hardcoded list.\")\n",
        "        place_ids = DEFAULT_PLACE_IDS\n",
        "\n",
        "    # Remove duplicates and empties if any\n",
        "    place_ids = [pid for pid in place_ids if pid]\n",
        "\n",
        "    print(f\"Processing {len(place_ids)} Place IDs...\")\n",
        "\n",
        "    # 2. Prepare Output CSV Headers\n",
        "    headers = [\n",
        "        \"place_id\", \"place_name\", \"city\", \"overall_rating\", \"total_ratings\", \"business_status\",\n",
        "        \"review1_author\", \"review1_rating\", \"review1_text\", \"review1_date\",\n",
        "        \"review2_author\", \"review2_rating\", \"review2_text\", \"review2_date\",\n",
        "        \"review3_author\", \"review3_rating\", \"review3_text\", \"review3_date\",\n",
        "        \"review4_author\", \"review4_rating\", \"review4_text\", \"review4_date\",\n",
        "        \"review5_author\", \"review5_rating\", \"review5_text\", \"review5_date\"\n",
        "    ]\n",
        "\n",
        "    # 3. Open Output File\n",
        "    with open(OUTPUT_FILE, mode='w', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(headers)\n",
        "\n",
        "        for pid in place_ids:\n",
        "            print(f\"Fetching data for: {pid}\")\n",
        "\n",
        "            details = get_place_details(pid)\n",
        "\n",
        "            # Construct the row\n",
        "            row = []\n",
        "\n",
        "            if details:\n",
        "                # Base info\n",
        "                row.extend([\n",
        "                    details[\"place_id\"],\n",
        "                    details[\"place_name\"],\n",
        "                    details[\"city\"],\n",
        "                    details[\"overall_rating\"],\n",
        "                    details[\"total_ratings\"],\n",
        "                    details[\"business_status\"]\n",
        "                ])\n",
        "\n",
        "                # Review info (loop 5 times)\n",
        "                reviews_list = details[\"reviews\"]\n",
        "                for i in range(5):\n",
        "                    if i < len(reviews_list):\n",
        "                        r = reviews_list[i]\n",
        "                        row.extend([\n",
        "                            r.get(\"author_name\", \"\"),\n",
        "                            r.get(\"rating\", \"\"),\n",
        "                            r.get(\"text\", \"\"),\n",
        "                            format_date(r.get(\"time\"))\n",
        "                        ])\n",
        "                    else:\n",
        "                        # Empty fields if fewer than 5 reviews\n",
        "                        row.extend([\"\", \"\", \"\", \"\"])\n",
        "            else:\n",
        "                # API Failed or Invalid ID -> Write ID and blanks\n",
        "                row.append(pid)\n",
        "                # Add 25 empty strings to complete the row\n",
        "                row.extend([\"\"] * 25)\n",
        "\n",
        "            writer.writerow(row)\n",
        "\n",
        "            # Rate Limiting Sleep\n",
        "            time.sleep(0.2)\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"Done. Data written to {OUTPUT_FILE}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdHKSjlWWQqh",
        "outputId": "5180f3ad-570e-4e3b-a651-cb6dff48dcb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No input CSV found. Using hardcoded list.\n",
            "Processing 2 Place IDs...\n",
            "Fetching data for: ChIJK7aICxoVrjsRnjZbF0TjvNo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3637190840.py:50: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
            "  return datetime.datetime.utcfromtimestamp(timestamp).strftime('%Y-%m-%d')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching data for: ChIJZU8k2vU9rjsRK9gXANkis7k\n",
            "------------------------------\n",
            "Done. Data written to places_reviews_output.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "import time\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "# ================= CONFIGURATION =================\n",
        "API_KEY = \"AIzaSyDlKfGNRGtlfBKe9X_lktXeM79pYPMiO0E\"  # <--- REPLACE THIS\n",
        "\n",
        "OUTPUT_FILE = \"places_newest_reviews.csv\"\n",
        "INPUT_CSV = \"input_place_ids.csv\"\n",
        "\n",
        "DEFAULT_PLACE_IDS = [\n",
        "    \"ChIJK7aICxoVrjsRnjZbF0TjvNo\",\n",
        "    \"ChIJZU8k2vU9rjsRK9gXANkis7k\",\n",
        "]\n",
        "\n",
        "BASE_URL = \"https://maps.googleapis.com/maps/api/place/details/json\"\n",
        "\n",
        "# ================= LOGIC =================\n",
        "\n",
        "def get_city_from_components(components):\n",
        "    if not components:\n",
        "        return \"\"\n",
        "    for comp in components:\n",
        "        if \"locality\" in comp.get(\"types\", []):\n",
        "            return comp.get(\"long_name\", \"\")\n",
        "    for comp in components:\n",
        "        if \"administrative_area_level_2\" in comp.get(\"types\", []):\n",
        "            return comp.get(\"long_name\", \"\")\n",
        "    return \"\"\n",
        "\n",
        "def format_date(timestamp):\n",
        "    \"\"\"Converts Unix timestamp to YYYY-MM-DD.\"\"\"\n",
        "    if not timestamp:\n",
        "        return \"\"\n",
        "    try:\n",
        "        # Using standard datetime from timestamp\n",
        "        return datetime.datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d')\n",
        "    except (ValueError, TypeError):\n",
        "        return \"\"\n",
        "\n",
        "def get_place_details(place_id):\n",
        "    params = {\n",
        "        \"place_id\": place_id,\n",
        "        \"key\": API_KEY,\n",
        "        # ATTEMPT TO FORCE NEWEST\n",
        "        # Note: This is undocumented/deprecated in the standard API\n",
        "        # but is the only possible way to attempt a server-side sort.\n",
        "        \"reviews_sort\": \"newest\",\n",
        "        \"language\": \"en\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(BASE_URL, params=params)\n",
        "        data = response.json()\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] HTTP Request failed for {place_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "    status = data.get(\"status\")\n",
        "\n",
        "    if status != \"OK\":\n",
        "        print(f\"[API ERROR] Place ID {place_id} returned status: {status}\")\n",
        "        return None\n",
        "\n",
        "    result = data.get(\"result\", {})\n",
        "\n",
        "    # Base Info\n",
        "    extracted_data = {\n",
        "        \"place_id\": place_id,\n",
        "        \"place_name\": result.get(\"name\", \"\"),\n",
        "        \"city\": get_city_from_components(result.get(\"address_components\", [])),\n",
        "        \"overall_rating\": result.get(\"rating\", \"\"),\n",
        "        \"total_ratings\": result.get(\"user_ratings_total\", \"\"),\n",
        "        \"business_status\": result.get(\"business_status\", \"\"),\n",
        "        \"reviews\": []\n",
        "    }\n",
        "\n",
        "    # Review Processing\n",
        "    raw_reviews = result.get(\"reviews\", [])\n",
        "\n",
        "    if raw_reviews:\n",
        "        # 1. Sort the received batch by time descending (Newest first)\n",
        "        # This ensures that OF THE 5 we got, the newest is listed first.\n",
        "        sorted_reviews = sorted(raw_reviews, key=lambda x: x.get(\"time\", 0), reverse=True)\n",
        "\n",
        "        # 2. Store top 5\n",
        "        extracted_data[\"reviews\"] = sorted_reviews[:5]\n",
        "\n",
        "    return extracted_data\n",
        "\n",
        "def main():\n",
        "    # 1. Load IDs\n",
        "    place_ids = []\n",
        "    if os.path.exists(INPUT_CSV):\n",
        "        print(f\"Reading Place IDs from {INPUT_CSV}...\")\n",
        "        try:\n",
        "            with open(INPUT_CSV, mode='r', encoding='utf-8-sig') as f:\n",
        "                reader = csv.DictReader(f)\n",
        "                for row in reader:\n",
        "                    if 'place_id' in row:\n",
        "                        place_ids.append(row['place_id'].strip())\n",
        "                    else:\n",
        "                        place_ids.append(list(row.values())[0].strip())\n",
        "        except Exception:\n",
        "            place_ids = DEFAULT_PLACE_IDS\n",
        "    else:\n",
        "        place_ids = DEFAULT_PLACE_IDS\n",
        "\n",
        "    place_ids = [pid for pid in place_ids if pid]\n",
        "    print(f\"Processing {len(place_ids)} Place IDs...\")\n",
        "\n",
        "    # 2. Prepare CSV Headers\n",
        "    headers = [\n",
        "        \"place_id\", \"place_name\", \"city\", \"overall_rating\", \"total_ratings\", \"business_status\",\n",
        "        \"review1_author\", \"review1_rating\", \"review1_text\", \"review1_date\",\n",
        "        \"review2_author\", \"review2_rating\", \"review2_text\", \"review2_date\",\n",
        "        \"review3_author\", \"review3_rating\", \"review3_text\", \"review3_date\",\n",
        "        \"review4_author\", \"review4_rating\", \"review4_text\", \"review4_date\",\n",
        "        \"review5_author\", \"review5_rating\", \"review5_text\", \"review5_date\"\n",
        "    ]\n",
        "\n",
        "    # 3. Write Output\n",
        "    with open(OUTPUT_FILE, mode='w', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(headers)\n",
        "\n",
        "        for pid in place_ids:\n",
        "            print(f\"Fetching data for: {pid}\")\n",
        "\n",
        "            details = get_place_details(pid)\n",
        "\n",
        "            row = []\n",
        "            if details:\n",
        "                row.extend([\n",
        "                    details[\"place_id\"], details[\"place_name\"], details[\"city\"],\n",
        "                    details[\"overall_rating\"], details[\"total_ratings\"], details[\"business_status\"]\n",
        "                ])\n",
        "\n",
        "                reviews_list = details[\"reviews\"]\n",
        "                for i in range(5):\n",
        "                    if i < len(reviews_list):\n",
        "                        r = reviews_list[i]\n",
        "                        row.extend([\n",
        "                            r.get(\"author_name\", \"\"),\n",
        "                            r.get(\"rating\", \"\"),\n",
        "                            r.get(\"text\", \"\"),\n",
        "                            format_date(r.get(\"time\"))\n",
        "                        ])\n",
        "                    else:\n",
        "                        row.extend([\"\", \"\", \"\", \"\"])\n",
        "            else:\n",
        "                row.append(pid)\n",
        "                row.extend([\"\"] * 25)\n",
        "\n",
        "            writer.writerow(row)\n",
        "            time.sleep(0.2) # Rate limit\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"Done. Data written to {OUTPUT_FILE}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzn3jAEwbeAC",
        "outputId": "5cad6bbc-fcd8-4eda-8bdb-b7cd4c9e09c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 2 Place IDs...\n",
            "Fetching data for: ChIJK7aICxoVrjsRnjZbF0TjvNo\n",
            "Fetching data for: ChIJZU8k2vU9rjsRK9gXANkis7k\n",
            "------------------------------\n",
            "Done. Data written to places_newest_reviews.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "import time\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "# ================= CONFIGURATION =================\n",
        "API_KEY = \"AIzaSyDlKfGNRGtlfBKe9X_lktXeM79pYPMiO0E\"  # <--- REPLACE THIS\n",
        "\n",
        "OUTPUT_FILE = \"places_reviews_output.csv\"\n",
        "INPUT_CSV = \"input_place_ids.csv\"\n",
        "\n",
        "# Hardcoded list (used if INPUT_CSV doesn't exist)\n",
        "DEFAULT_PLACE_IDS = [\n",
        "    \"ChIJK7aICxoVrjsRnjZbF0TjvNo\",\n",
        "    \"ChIJZU8k2vU9rjsRK9gXANkis7k\",\n",
        "]\n",
        "\n",
        "BASE_URL = \"https://maps.googleapis.com/maps/api/place/details/json\"\n",
        "\n",
        "# ================= LOGIC =================\n",
        "\n",
        "def get_city_from_components(components):\n",
        "    if not components:\n",
        "        return \"\"\n",
        "    # First pass: Locality\n",
        "    for comp in components:\n",
        "        if \"locality\" in comp.get(\"types\", []):\n",
        "            return comp.get(\"long_name\", \"\")\n",
        "    # Second pass: Administrative Area Level 2\n",
        "    for comp in components:\n",
        "        if \"administrative_area_level_2\" in comp.get(\"types\", []):\n",
        "            return comp.get(\"long_name\", \"\")\n",
        "    return \"\"\n",
        "\n",
        "def format_date(timestamp):\n",
        "    \"\"\"Converts Unix timestamp to YYYY-MM-DD.\"\"\"\n",
        "    if not timestamp:\n",
        "        return \"\"\n",
        "    try:\n",
        "        return datetime.datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d')\n",
        "    except (ValueError, TypeError):\n",
        "        return \"\"\n",
        "\n",
        "def get_place_details(place_id):\n",
        "    params = {\n",
        "        \"place_id\": place_id,\n",
        "        \"key\": API_KEY,\n",
        "        \"reviews_sort\": \"newest\", # Attempt to force newest server-side\n",
        "        \"language\": \"en\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(BASE_URL, params=params)\n",
        "        data = response.json()\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] HTTP Request failed for {place_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "    status = data.get(\"status\")\n",
        "\n",
        "    if status != \"OK\":\n",
        "        print(f\"[API ERROR] Place ID {place_id} returned status: {status}\")\n",
        "        return None\n",
        "\n",
        "    result = data.get(\"result\", {})\n",
        "\n",
        "    # Base Info\n",
        "    extracted_data = {\n",
        "        \"place_id\": place_id,\n",
        "        \"place_name\": result.get(\"name\", \"\"),\n",
        "        \"city\": get_city_from_components(result.get(\"address_components\", [])),\n",
        "        \"overall_rating\": result.get(\"rating\", \"\"),\n",
        "        \"total_ratings\": result.get(\"user_ratings_total\", \"\"),\n",
        "        \"business_status\": result.get(\"business_status\", \"\"),\n",
        "        \"reviews\": []\n",
        "    }\n",
        "\n",
        "    # Review Processing\n",
        "    raw_reviews = result.get(\"reviews\", [])\n",
        "\n",
        "    if raw_reviews:\n",
        "        # Sort by time descending (Newest first)\n",
        "        sorted_reviews = sorted(raw_reviews, key=lambda x: x.get(\"time\", 0), reverse=True)\n",
        "        extracted_data[\"reviews\"] = sorted_reviews[:5]\n",
        "\n",
        "    return extracted_data\n",
        "\n",
        "def main():\n",
        "    # 1. Get Current Date for the Run\n",
        "    current_run_date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "    # 2. Load IDs\n",
        "    place_ids = []\n",
        "    if os.path.exists(INPUT_CSV):\n",
        "        print(f\"Reading Place IDs from {INPUT_CSV}...\")\n",
        "        try:\n",
        "            with open(INPUT_CSV, mode='r', encoding='utf-8-sig') as f:\n",
        "                reader = csv.DictReader(f)\n",
        "                for row in reader:\n",
        "                    if 'place_id' in row:\n",
        "                        place_ids.append(row['place_id'].strip())\n",
        "                    else:\n",
        "                        # Fallback to first column\n",
        "                        place_ids.append(list(row.values())[0].strip())\n",
        "        except Exception:\n",
        "            place_ids = DEFAULT_PLACE_IDS\n",
        "    else:\n",
        "        place_ids = DEFAULT_PLACE_IDS\n",
        "\n",
        "    place_ids = [pid for pid in place_ids if pid]\n",
        "    print(f\"Processing {len(place_ids)} Place IDs...\")\n",
        "\n",
        "    # 3. Prepare CSV Headers (Added 'run_date' at the start)\n",
        "    headers = [\n",
        "        \"run_date\", \"place_id\", \"place_name\", \"city\", \"overall_rating\", \"total_ratings\", \"business_status\",\n",
        "        \"review1_author\", \"review1_rating\", \"review1_text\", \"review1_date\",\n",
        "        \"review2_author\", \"review2_rating\", \"review2_text\", \"review2_date\",\n",
        "        \"review3_author\", \"review3_rating\", \"review3_text\", \"review3_date\",\n",
        "        \"review4_author\", \"review4_rating\", \"review4_text\", \"review4_date\",\n",
        "        \"review5_author\", \"review5_rating\", \"review5_text\", \"review5_date\"\n",
        "    ]\n",
        "\n",
        "    # 4. Write Output\n",
        "    with open(OUTPUT_FILE, mode='w', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(headers)\n",
        "\n",
        "        for pid in place_ids:\n",
        "            print(f\"Fetching data for: {pid}\")\n",
        "\n",
        "            details = get_place_details(pid)\n",
        "\n",
        "            # Start the row with the Run Date\n",
        "            row = [current_run_date]\n",
        "\n",
        "            if details:\n",
        "                row.extend([\n",
        "                    details[\"place_id\"], details[\"place_name\"], details[\"city\"],\n",
        "                    details[\"overall_rating\"], details[\"total_ratings\"], details[\"business_status\"]\n",
        "                ])\n",
        "\n",
        "                reviews_list = details[\"reviews\"]\n",
        "                for i in range(5):\n",
        "                    if i < len(reviews_list):\n",
        "                        r = reviews_list[i]\n",
        "                        row.extend([\n",
        "                            r.get(\"author_name\", \"\"),\n",
        "                            r.get(\"rating\", \"\"),\n",
        "                            r.get(\"text\", \"\"),\n",
        "                            format_date(r.get(\"time\"))\n",
        "                        ])\n",
        "                    else:\n",
        "                        row.extend([\"\", \"\", \"\", \"\"])\n",
        "            else:\n",
        "                # If API fails, still write run_date and place_id, leave rest blank\n",
        "                row.append(pid)\n",
        "                row.extend([\"\"] * 25)\n",
        "\n",
        "            writer.writerow(row)\n",
        "            time.sleep(0.2) # Rate limit\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"Done. Data written to {OUTPUT_FILE}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TY7GOHfWdYj3",
        "outputId": "22110bb6-b1ff-4dcf-a894-b2fd6bdecb2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 2 Place IDs...\n",
            "Fetching data for: ChIJK7aICxoVrjsRnjZbF0TjvNo\n",
            "Fetching data for: ChIJZU8k2vU9rjsRK9gXANkis7k\n",
            "------------------------------\n",
            "Done. Data written to places_reviews_output.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "import time\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "# ================= CONFIGURATION =================\n",
        "API_KEY = \"AIzaSyDlKfGNRGtlfBKe9X_lktXeM79pYPMiO0E\"  # <--- REPLACE THIS\n",
        "\n",
        "OUTPUT_FILE = \"places_reviews_long_format.csv\"\n",
        "INPUT_CSV = \"input_place_ids.csv\"\n",
        "\n",
        "DEFAULT_PLACE_IDS = [\n",
        "    \"ChIJK7aICxoVrjsRnjZbF0TjvNo\",\n",
        "    \"ChIJZU8k2vU9rjsRK9gXANkis7k\",\n",
        "]\n",
        "\n",
        "BASE_URL = \"https://maps.googleapis.com/maps/api/place/details/json\"\n",
        "\n",
        "# ================= LOGIC =================\n",
        "\n",
        "def get_city_from_components(components):\n",
        "    if not components:\n",
        "        return \"\"\n",
        "    for comp in components:\n",
        "        if \"locality\" in comp.get(\"types\", []):\n",
        "            return comp.get(\"long_name\", \"\")\n",
        "    for comp in components:\n",
        "        if \"administrative_area_level_2\" in comp.get(\"types\", []):\n",
        "            return comp.get(\"long_name\", \"\")\n",
        "    return \"\"\n",
        "\n",
        "def format_date(timestamp):\n",
        "    if not timestamp:\n",
        "        return \"\"\n",
        "    try:\n",
        "        return datetime.datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d')\n",
        "    except (ValueError, TypeError):\n",
        "        return \"\"\n",
        "\n",
        "def get_place_details(place_id):\n",
        "    params = {\n",
        "        \"place_id\": place_id,\n",
        "        \"key\": API_KEY,\n",
        "        \"reviews_sort\": \"newest\",\n",
        "        \"language\": \"en\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(BASE_URL, params=params)\n",
        "        data = response.json()\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] HTTP Request failed for {place_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "    status = data.get(\"status\")\n",
        "    if status != \"OK\":\n",
        "        print(f\"[API ERROR] Place ID {place_id} returned status: {status}\")\n",
        "        return None\n",
        "\n",
        "    result = data.get(\"result\", {})\n",
        "\n",
        "    # Base Info\n",
        "    extracted_data = {\n",
        "        \"place_id\": place_id,\n",
        "        \"place_name\": result.get(\"name\", \"\"),\n",
        "        \"city\": get_city_from_components(result.get(\"address_components\", [])),\n",
        "        \"overall_rating\": result.get(\"rating\", \"\"),\n",
        "        \"total_ratings\": result.get(\"user_ratings_total\", \"\"),\n",
        "        \"business_status\": result.get(\"business_status\", \"\"),\n",
        "        \"reviews\": []\n",
        "    }\n",
        "\n",
        "    # Review Processing\n",
        "    raw_reviews = result.get(\"reviews\", [])\n",
        "    if raw_reviews:\n",
        "        sorted_reviews = sorted(raw_reviews, key=lambda x: x.get(\"time\", 0), reverse=True)\n",
        "        # Keep top 5\n",
        "        extracted_data[\"reviews\"] = sorted_reviews[:5]\n",
        "\n",
        "    return extracted_data\n",
        "\n",
        "def main():\n",
        "    current_run_date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "    # 1. Load IDs\n",
        "    place_ids = []\n",
        "    if os.path.exists(INPUT_CSV):\n",
        "        print(f\"Reading Place IDs from {INPUT_CSV}...\")\n",
        "        try:\n",
        "            with open(INPUT_CSV, mode='r', encoding='utf-8-sig') as f:\n",
        "                reader = csv.DictReader(f)\n",
        "                for row in reader:\n",
        "                    if 'place_id' in row:\n",
        "                        place_ids.append(row['place_id'].strip())\n",
        "                    else:\n",
        "                        place_ids.append(list(row.values())[0].strip())\n",
        "        except Exception:\n",
        "            place_ids = DEFAULT_PLACE_IDS\n",
        "    else:\n",
        "        place_ids = DEFAULT_PLACE_IDS\n",
        "\n",
        "    place_ids = [pid for pid in place_ids if pid]\n",
        "    print(f\"Processing {len(place_ids)} Place IDs...\")\n",
        "\n",
        "    # 2. Prepare CSV Headers (Standardized for Long Format)\n",
        "    headers = [\n",
        "        \"run_date\",\n",
        "        \"place_id\",\n",
        "        \"place_name\",\n",
        "        \"city\",\n",
        "        \"overall_rating\",\n",
        "        \"total_ratings\",\n",
        "        \"business_status\",\n",
        "        \"author_name\",  # Review specific columns start here\n",
        "        \"rating\",\n",
        "        \"text\",\n",
        "        \"review_date\"\n",
        "    ]\n",
        "\n",
        "    # 3. Write Output\n",
        "    with open(OUTPUT_FILE, mode='w', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(headers)\n",
        "\n",
        "        for pid in place_ids:\n",
        "            print(f\"Fetching data for: {pid}\")\n",
        "\n",
        "            details = get_place_details(pid)\n",
        "\n",
        "            if details:\n",
        "                # Create the \"Base\" part of the row (Common for all reviews of this place)\n",
        "                base_row = [\n",
        "                    current_run_date,\n",
        "                    details[\"place_id\"],\n",
        "                    details[\"place_name\"],\n",
        "                    details[\"city\"],\n",
        "                    details[\"overall_rating\"],\n",
        "                    details[\"total_ratings\"],\n",
        "                    details[\"business_status\"]\n",
        "                ]\n",
        "\n",
        "                reviews_list = details[\"reviews\"]\n",
        "\n",
        "                if reviews_list:\n",
        "                    # CASE A: Reviews exist -> Write one row per review\n",
        "                    for r in reviews_list:\n",
        "                        full_row = base_row + [\n",
        "                            r.get(\"author_name\", \"\"),\n",
        "                            r.get(\"rating\", \"\"),\n",
        "                            r.get(\"text\", \"\"),\n",
        "                            format_date(r.get(\"time\"))\n",
        "                        ]\n",
        "                        writer.writerow(full_row)\n",
        "                else:\n",
        "                    # CASE B: No reviews exist -> Write one row with Place info + Blanks\n",
        "                    full_row = base_row + [\"\", \"\", \"\", \"\"]\n",
        "                    writer.writerow(full_row)\n",
        "\n",
        "            else:\n",
        "                # CASE C: API Failed -> Write run_date, place_id + Blanks\n",
        "                error_row = [current_run_date, pid] + [\"\"] * 9\n",
        "                writer.writerow(error_row)\n",
        "\n",
        "            time.sleep(0.2) # Rate limit\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"Done. Data written to {OUTPUT_FILE}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzGbu_Z7-oUq",
        "outputId": "65bd0bda-34b7-4698-fa57-7ff73489bc61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 2 Place IDs...\n",
            "Fetching data for: ChIJK7aICxoVrjsRnjZbF0TjvNo\n",
            "Fetching data for: ChIJZU8k2vU9rjsRK9gXANkis7k\n",
            "------------------------------\n",
            "Done. Data written to places_reviews_long_format.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "import time\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "# ================= CONFIGURATION =================\n",
        "API_KEY = \"YOUR_GOOGLE_MAPS_API_KEY\"  # <--- REPLACE THIS\n",
        "\n",
        "# INPUT: The file containing City (Col 1) and Place ID (Col 2)\n",
        "INPUT_CSV_PATH = \"/content/places_reviews_long_format.csv\"\n",
        "\n",
        "# OUTPUT: Where the full review data will be saved\n",
        "OUTPUT_CSV_PATH = \"/content/final_reviews_output.csv\"\n",
        "\n",
        "BASE_URL = \"https://maps.googleapis.com/maps/api/place/details/json\"\n",
        "\n",
        "# ================= LOGIC =================\n",
        "\n",
        "def get_city_from_components(components):\n",
        "    if not components:\n",
        "        return \"\"\n",
        "    for comp in components:\n",
        "        if \"locality\" in comp.get(\"types\", []):\n",
        "            return comp.get(\"long_name\", \"\")\n",
        "    for comp in components:\n",
        "        if \"administrative_area_level_2\" in comp.get(\"types\", []):\n",
        "            return comp.get(\"long_name\", \"\")\n",
        "    return \"\"\n",
        "\n",
        "def format_date(timestamp):\n",
        "    if not timestamp:\n",
        "        return \"\"\n",
        "    try:\n",
        "        return datetime.datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d')\n",
        "    except (ValueError, TypeError):\n",
        "        return \"\"\n",
        "\n",
        "def get_existing_signatures(filename):\n",
        "    \"\"\"\n",
        "    Reads the OUTPUT CSV to check for duplicates.\n",
        "    Signature = (place_id, author_name, rating, text, review_date)\n",
        "    \"\"\"\n",
        "    signatures = set()\n",
        "    if not os.path.isfile(filename):\n",
        "        return signatures\n",
        "\n",
        "    try:\n",
        "        with open(filename, mode='r', encoding='utf-8') as f:\n",
        "            reader = csv.DictReader(f)\n",
        "            for row in reader:\n",
        "                sig = (\n",
        "                    row.get(\"place_id\", \"\").strip(),\n",
        "                    row.get(\"author_name\", \"\").strip(),\n",
        "                    row.get(\"rating\", \"\").strip(),\n",
        "                    row.get(\"text\", \"\").strip(),\n",
        "                    row.get(\"review_date\", \"\").strip()\n",
        "                )\n",
        "                signatures.add(sig)\n",
        "    except Exception as e:\n",
        "        print(f\"Note: Output file exists but could not be read for deduplication: {e}\")\n",
        "\n",
        "    return signatures\n",
        "\n",
        "def get_place_details(place_id):\n",
        "    params = {\n",
        "        \"place_id\": place_id,\n",
        "        \"key\": API_KEY,\n",
        "        \"reviews_sort\": \"newest\",\n",
        "        \"language\": \"en\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(BASE_URL, params=params)\n",
        "        data = response.json()\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] HTTP Request failed for {place_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "    status = data.get(\"status\")\n",
        "    if status != \"OK\":\n",
        "        print(f\"[API ERROR] Place ID {place_id} returned status: {status}\")\n",
        "        return None\n",
        "\n",
        "    result = data.get(\"result\", {})\n",
        "\n",
        "    extracted_data = {\n",
        "        \"place_id\": place_id,\n",
        "        \"place_name\": result.get(\"name\", \"\"),\n",
        "        \"city\": get_city_from_components(result.get(\"address_components\", [])),\n",
        "        \"overall_rating\": result.get(\"rating\", \"\"),\n",
        "        \"total_ratings\": result.get(\"user_ratings_total\", \"\"),\n",
        "        \"business_status\": result.get(\"business_status\", \"\"),\n",
        "        \"reviews\": []\n",
        "    }\n",
        "\n",
        "    raw_reviews = result.get(\"reviews\", [])\n",
        "    if raw_reviews:\n",
        "        sorted_reviews = sorted(raw_reviews, key=lambda x: x.get(\"time\", 0), reverse=True)\n",
        "        extracted_data[\"reviews\"] = sorted_reviews[:5]\n",
        "\n",
        "    return extracted_data\n",
        "\n",
        "def main():\n",
        "    current_run_date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "    # 1. Load IDs from the specified input path\n",
        "    place_ids = []\n",
        "    if os.path.exists(INPUT_CSV_PATH):\n",
        "        print(f\"Reading Place IDs from {INPUT_CSV_PATH}...\")\n",
        "        try:\n",
        "            with open(INPUT_CSV_PATH, mode='r', encoding='utf-8-sig') as f:\n",
        "                # We use csv.reader to access columns by Index\n",
        "                reader = csv.reader(f)\n",
        "\n",
        "                # Skip the header row\n",
        "                next(reader, None)\n",
        "\n",
        "                for row in reader:\n",
        "                    # Ensure row has at least 2 columns\n",
        "                    if len(row) >= 2:\n",
        "                        # Column 0 is City, Column 1 is Place ID\n",
        "                        pid = row[1].strip()\n",
        "                        if pid:\n",
        "                            place_ids.append(pid)\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading input CSV: {e}\")\n",
        "            return\n",
        "    else:\n",
        "        print(f\"ERROR: Input file not found at {INPUT_CSV_PATH}\")\n",
        "        return\n",
        "\n",
        "    # Remove any empty entries\n",
        "    place_ids = [pid for pid in place_ids if pid]\n",
        "    print(f\"Found {len(place_ids)} Place IDs to process.\")\n",
        "\n",
        "    # 2. Load Existing Signatures from the OUTPUT file (to prevent duplicates)\n",
        "    existing_signatures = get_existing_signatures(OUTPUT_CSV_PATH)\n",
        "    print(f\"Found {len(existing_signatures)} existing reviews in output file.\")\n",
        "\n",
        "    # 3. Prepare Output Headers\n",
        "    headers = [\n",
        "        \"run_date\",\n",
        "        \"place_id\",\n",
        "        \"place_name\",\n",
        "        \"city\",\n",
        "        \"overall_rating\",\n",
        "        \"total_ratings\",\n",
        "        \"business_status\",\n",
        "        \"author_name\",\n",
        "        \"rating\",\n",
        "        \"text\",\n",
        "        \"review_date\"\n",
        "    ]\n",
        "\n",
        "    file_exists = os.path.isfile(OUTPUT_CSV_PATH)\n",
        "\n",
        "    # 4. Open Output File (Append Mode)\n",
        "    print(f\"Writing data to {OUTPUT_CSV_PATH}...\")\n",
        "    with open(OUTPUT_CSV_PATH, mode='a', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "\n",
        "        # Write headers only if file is new\n",
        "        if not file_exists:\n",
        "            writer.writerow(headers)\n",
        "\n",
        "        new_reviews_count = 0\n",
        "\n",
        "        for pid in place_ids:\n",
        "            print(f\"Fetching data for: {pid}\")\n",
        "\n",
        "            details = get_place_details(pid)\n",
        "\n",
        "            if details:\n",
        "                base_row = [\n",
        "                    current_run_date,\n",
        "                    details[\"place_id\"],\n",
        "                    details[\"place_name\"],\n",
        "                    details[\"city\"],\n",
        "                    details[\"overall_rating\"],\n",
        "                    details[\"total_ratings\"],\n",
        "                    details[\"business_status\"]\n",
        "                ]\n",
        "\n",
        "                reviews_list = details[\"reviews\"]\n",
        "\n",
        "                if reviews_list:\n",
        "                    for r in reviews_list:\n",
        "                        r_author = r.get(\"author_name\", \"\").strip()\n",
        "                        r_rating = str(r.get(\"rating\", \"\")).strip()\n",
        "                        r_text = r.get(\"text\", \"\").strip()\n",
        "                        r_date = format_date(r.get(\"time\")).strip()\n",
        "\n",
        "                        # Duplicate Check\n",
        "                        current_signature = (pid, r_author, r_rating, r_text, r_date)\n",
        "\n",
        "                        if current_signature in existing_signatures:\n",
        "                            continue\n",
        "\n",
        "                        # Write new unique review\n",
        "                        full_row = base_row + [r_author, r_rating, r_text, r_date]\n",
        "                        writer.writerow(full_row)\n",
        "\n",
        "                        existing_signatures.add(current_signature)\n",
        "                        new_reviews_count += 1\n",
        "                else:\n",
        "                    # Write one row if no reviews exist\n",
        "                    full_row = base_row + [\"\", \"\", \"\", \"\"]\n",
        "                    writer.writerow(full_row)\n",
        "\n",
        "            else:\n",
        "                # API Error\n",
        "                error_row = [current_run_date, pid] + [\"\"] * 9\n",
        "                writer.writerow(error_row)\n",
        "\n",
        "            time.sleep(0.2) # Rate limit\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"Done. Added {new_reviews_count} new reviews to {OUTPUT_CSV_PATH}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "RZ3g8UgDGM2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "import time\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "# ================= CONFIGURATION =================\n",
        "API_KEY = \"AIzaSyDlKfGNRGtlfBKe9X_lktXeM79pYPMiO0E\"  # <--- REPLACE THIS\n",
        "\n",
        "# INPUT: The file containing City (Col 1) and Place ID (Col 2)\n",
        "INPUT_CSV_PATH = \"/content/places_reviews_long_format.csv\"\n",
        "\n",
        "# OUTPUT: Where the full review data will be saved\n",
        "OUTPUT_CSV_PATH = \"/content/final_reviews_output.csv\"\n",
        "\n",
        "BASE_URL = \"https://maps.googleapis.com/maps/api/place/details/json\"\n",
        "\n",
        "# ================= LOGIC =================\n",
        "\n",
        "def get_city_from_components(components):\n",
        "    if not components:\n",
        "        return \"\"\n",
        "    for comp in components:\n",
        "        if \"locality\" in comp.get(\"types\", []):\n",
        "            return comp.get(\"long_name\", \"\")\n",
        "    for comp in components:\n",
        "        if \"administrative_area_level_2\" in comp.get(\"types\", []):\n",
        "            return comp.get(\"long_name\", \"\")\n",
        "    return \"\"\n",
        "\n",
        "def format_date(timestamp):\n",
        "    if not timestamp:\n",
        "        return \"\"\n",
        "    try:\n",
        "        return datetime.datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d')\n",
        "    except (ValueError, TypeError):\n",
        "        return \"\"\n",
        "\n",
        "def get_existing_signatures(filename):\n",
        "    \"\"\"\n",
        "    Reads the OUTPUT CSV to check for duplicates.\n",
        "    Signature = (place_id, author_name, rating, text, review_date)\n",
        "    \"\"\"\n",
        "    signatures = set()\n",
        "    if not os.path.isfile(filename):\n",
        "        return signatures\n",
        "\n",
        "    try:\n",
        "        with open(filename, mode='r', encoding='utf-8') as f:\n",
        "            reader = csv.DictReader(f)\n",
        "            for row in reader:\n",
        "                sig = (\n",
        "                    row.get(\"place_id\", \"\").strip(),\n",
        "                    row.get(\"author_name\", \"\").strip(),\n",
        "                    row.get(\"rating\", \"\").strip(),\n",
        "                    row.get(\"text\", \"\").strip(),\n",
        "                    row.get(\"review_date\", \"\").strip()\n",
        "                )\n",
        "                signatures.add(sig)\n",
        "    except Exception as e:\n",
        "        print(f\"Note: Output file exists but could not be read for deduplication: {e}\")\n",
        "\n",
        "    return signatures\n",
        "\n",
        "def get_place_details(place_id):\n",
        "    params = {\n",
        "        \"place_id\": place_id,\n",
        "        \"key\": API_KEY,\n",
        "        \"reviews_sort\": \"newest\",\n",
        "        \"language\": \"en\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(BASE_URL, params=params)\n",
        "        data = response.json()\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] HTTP Request failed for {place_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "    status = data.get(\"status\")\n",
        "    if status != \"OK\":\n",
        "        print(f\"[API ERROR] Place ID {place_id} returned status: {status}\")\n",
        "        return None\n",
        "\n",
        "    result = data.get(\"result\", {})\n",
        "\n",
        "    extracted_data = {\n",
        "        \"place_id\": place_id,\n",
        "        \"place_name\": result.get(\"name\", \"\"),\n",
        "        \"city\": get_city_from_components(result.get(\"address_components\", [])),\n",
        "        \"overall_rating\": result.get(\"rating\", \"\"),\n",
        "        \"total_ratings\": result.get(\"user_ratings_total\", \"\"),\n",
        "        \"business_status\": result.get(\"business_status\", \"\"),\n",
        "        \"reviews\": []\n",
        "    }\n",
        "\n",
        "    raw_reviews = result.get(\"reviews\", [])\n",
        "    if raw_reviews:\n",
        "        sorted_reviews = sorted(raw_reviews, key=lambda x: x.get(\"time\", 0), reverse=True)\n",
        "        extracted_data[\"reviews\"] = sorted_reviews[:5]\n",
        "\n",
        "    return extracted_data\n",
        "\n",
        "def main():\n",
        "    current_run_date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "    # 1. Load IDs from the specified input path\n",
        "    place_ids = []\n",
        "    if os.path.exists(INPUT_CSV_PATH):\n",
        "        print(f\"Reading Place IDs from {INPUT_CSV_PATH}...\")\n",
        "        try:\n",
        "            with open(INPUT_CSV_PATH, mode='r', encoding='utf-8-sig') as f:\n",
        "                # We use csv.reader to access columns by Index\n",
        "                reader = csv.reader(f)\n",
        "\n",
        "                # Skip the header row\n",
        "                next(reader, None)\n",
        "\n",
        "                for row in reader:\n",
        "                    # Ensure row has at least 2 columns\n",
        "                    if len(row) >= 2:\n",
        "                        # Column 0 is City, Column 1 is Place ID\n",
        "                        pid = row[1].strip()\n",
        "                        if pid:\n",
        "                            place_ids.append(pid)\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading input CSV: {e}\")\n",
        "            return\n",
        "    else:\n",
        "        print(f\"ERROR: Input file not found at {INPUT_CSV_PATH}\")\n",
        "        return\n",
        "\n",
        "    # Remove any empty entries\n",
        "    place_ids = [pid for pid in place_ids if pid]\n",
        "    print(f\"Found {len(place_ids)} Place IDs to process.\")\n",
        "\n",
        "    # 2. Load Existing Signatures from the OUTPUT file (to prevent duplicates)\n",
        "    existing_signatures = get_existing_signatures(OUTPUT_CSV_PATH)\n",
        "    print(f\"Found {len(existing_signatures)} existing reviews in output file.\")\n",
        "\n",
        "    # 3. Prepare Output Headers\n",
        "    headers = [\n",
        "        \"run_date\",\n",
        "        \"place_id\",\n",
        "        \"place_name\",\n",
        "        \"city\",\n",
        "        \"overall_rating\",\n",
        "        \"total_ratings\",\n",
        "        \"business_status\",\n",
        "        \"author_name\",\n",
        "        \"rating\",\n",
        "        \"text\",\n",
        "        \"review_date\"\n",
        "    ]\n",
        "\n",
        "    file_exists = os.path.isfile(OUTPUT_CSV_PATH)\n",
        "\n",
        "    # 4. Open Output File (Append Mode)\n",
        "    print(f\"Writing data to {OUTPUT_CSV_PATH}...\")\n",
        "    with open(OUTPUT_CSV_PATH, mode='a', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "\n",
        "        # Write headers only if file is new\n",
        "        if not file_exists:\n",
        "            writer.writerow(headers)\n",
        "\n",
        "        new_reviews_count = 0\n",
        "\n",
        "        for pid in place_ids:\n",
        "            print(f\"Fetching data for: {pid}\")\n",
        "\n",
        "            details = get_place_details(pid)\n",
        "\n",
        "            if details:\n",
        "                base_row = [\n",
        "                    current_run_date,\n",
        "                    details[\"place_id\"],\n",
        "                    details[\"place_name\"],\n",
        "                    details[\"city\"],\n",
        "                    details[\"overall_rating\"],\n",
        "                    details[\"total_ratings\"],\n",
        "                    details[\"business_status\"]\n",
        "                ]\n",
        "\n",
        "                reviews_list = details[\"reviews\"]\n",
        "\n",
        "                if reviews_list:\n",
        "                    for r in reviews_list:\n",
        "                        r_author = r.get(\"author_name\", \"\").strip()\n",
        "                        r_rating = str(r.get(\"rating\", \"\")).strip()\n",
        "                        r_text = r.get(\"text\", \"\").strip()\n",
        "                        r_date = format_date(r.get(\"time\")).strip()\n",
        "\n",
        "                        # Duplicate Check\n",
        "                        current_signature = (pid, r_author, r_rating, r_text, r_date)\n",
        "\n",
        "                        if current_signature in existing_signatures:\n",
        "                            continue\n",
        "\n",
        "                        # Write new unique review\n",
        "                        full_row = base_row + [r_author, r_rating, r_text, r_date]\n",
        "                        writer.writerow(full_row)\n",
        "\n",
        "                        existing_signatures.add(current_signature)\n",
        "                        new_reviews_count += 1\n",
        "                else:\n",
        "                    # Write one row if no reviews exist\n",
        "                    full_row = base_row + [\"\", \"\", \"\", \"\"]\n",
        "                    writer.writerow(full_row)\n",
        "\n",
        "            else:\n",
        "                # API Error\n",
        "                error_row = [current_run_date, pid] + [\"\"] * 9\n",
        "                writer.writerow(error_row)\n",
        "\n",
        "            time.sleep(0.2) # Rate limit\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"Done. Added {new_reviews_count} new reviews to {OUTPUT_CSV_PATH}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cF_sQSP-GUWL",
        "outputId": "b832d451-4e9f-4d2e-aadf-1abc9338b349"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading Place IDs from /content/places_reviews_long_format.csv...\n",
            "Found 10 Place IDs to process.\n",
            "Found 0 existing reviews in output file.\n",
            "Writing data to /content/final_reviews_output.csv...\n",
            "Fetching data for: ChIJK7aICxoVrjsRnjZbF0TjvNo\n",
            "Fetching data for: ChIJK7aICxoVrjsRnjZbF0TjvNo\n",
            "Fetching data for: ChIJK7aICxoVrjsRnjZbF0TjvNo\n",
            "Fetching data for: ChIJK7aICxoVrjsRnjZbF0TjvNo\n",
            "Fetching data for: ChIJK7aICxoVrjsRnjZbF0TjvNo\n",
            "Fetching data for: ChIJZU8k2vU9rjsRK9gXANkis7k\n",
            "Fetching data for: ChIJZU8k2vU9rjsRK9gXANkis7k\n",
            "Fetching data for: ChIJZU8k2vU9rjsRK9gXANkis7k\n",
            "Fetching data for: ChIJZU8k2vU9rjsRK9gXANkis7k\n",
            "Fetching data for: ChIJZU8k2vU9rjsRK9gXANkis7k\n",
            "------------------------------\n",
            "Done. Added 10 new reviews to /content/final_reviews_output.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "import time\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "# ================= CONFIGURATION =================\n",
        "API_KEY = \"AIzaSyDlKfGNRGtlfBKe9X_lktXeM79pYPMiO0E\"  # <--- REPLACE THIS\n",
        "\n",
        "OUTPUT_FILE = \"places_reviews_long_format.csv\"\n",
        "INPUT_CSV = \"input_place_ids.csv\"\n",
        "\n",
        "DEFAULT_PLACE_IDS = [\n",
        "    \"ChIJZU8k2vU9rjsRK9gXANkis7k\",\n",
        "    \"ChIJK7aICxoVrjsRnjZbF0TjvNo\",\n",
        "]\n",
        "\n",
        "BASE_URL = \"https://maps.googleapis.com/maps/api/place/details/json\"\n",
        "\n",
        "# ================= LOGIC =================\n",
        "\n",
        "def get_city_from_components(components):\n",
        "    if not components:\n",
        "        return \"\"\n",
        "    for comp in components:\n",
        "        if \"locality\" in comp.get(\"types\", []):\n",
        "            return comp.get(\"long_name\", \"\")\n",
        "    for comp in components:\n",
        "        if \"administrative_area_level_2\" in comp.get(\"types\", []):\n",
        "            return comp.get(\"long_name\", \"\")\n",
        "    return \"\"\n",
        "\n",
        "def format_date(timestamp):\n",
        "    if not timestamp:\n",
        "        return \"\"\n",
        "    try:\n",
        "        return datetime.datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d')\n",
        "    except (ValueError, TypeError):\n",
        "        return \"\"\n",
        "\n",
        "def get_existing_signatures(filename):\n",
        "    \"\"\"\n",
        "    Reads the existing CSV and returns a set of unique signatures.\n",
        "    Signature = (place_id, author_name, rating, text, review_date)\n",
        "    \"\"\"\n",
        "    signatures = set()\n",
        "    if not os.path.isfile(filename):\n",
        "        return signatures\n",
        "\n",
        "    try:\n",
        "        with open(filename, mode='r', encoding='utf-8') as f:\n",
        "            reader = csv.DictReader(f)\n",
        "            for row in reader:\n",
        "                # specific columns that define a unique review\n",
        "                sig = (\n",
        "                    row.get(\"place_id\", \"\").strip(),\n",
        "                    row.get(\"author_name\", \"\").strip(),\n",
        "                    row.get(\"rating\", \"\").strip(),\n",
        "                    row.get(\"text\", \"\").strip(),\n",
        "                    row.get(\"review_date\", \"\").strip()\n",
        "                )\n",
        "                signatures.add(sig)\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not read existing file for deduplication: {e}\")\n",
        "\n",
        "    return signatures\n",
        "\n",
        "def get_place_details(place_id):\n",
        "    params = {\n",
        "        \"place_id\": place_id,\n",
        "        \"key\": API_KEY,\n",
        "        \"reviews_sort\": \"newest\",\n",
        "        \"language\": \"en\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(BASE_URL, params=params)\n",
        "        data = response.json()\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] HTTP Request failed for {place_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "    status = data.get(\"status\")\n",
        "    if status != \"OK\":\n",
        "        print(f\"[API ERROR] Place ID {place_id} returned status: {status}\")\n",
        "        return None\n",
        "\n",
        "    result = data.get(\"result\", {})\n",
        "\n",
        "    extracted_data = {\n",
        "        \"place_id\": place_id,\n",
        "        \"place_name\": result.get(\"name\", \"\"),\n",
        "        \"city\": get_city_from_components(result.get(\"address_components\", [])),\n",
        "        \"overall_rating\": result.get(\"rating\", \"\"),\n",
        "        \"total_ratings\": result.get(\"user_ratings_total\", \"\"),\n",
        "        \"business_status\": result.get(\"business_status\", \"\"),\n",
        "        \"reviews\": []\n",
        "    }\n",
        "\n",
        "    raw_reviews = result.get(\"reviews\", [])\n",
        "    if raw_reviews:\n",
        "        sorted_reviews = sorted(raw_reviews, key=lambda x: x.get(\"time\", 0), reverse=True)\n",
        "        extracted_data[\"reviews\"] = sorted_reviews[:5]\n",
        "\n",
        "    return extracted_data\n",
        "\n",
        "def main():\n",
        "    current_run_date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "    # 1. Load IDs\n",
        "    place_ids = []\n",
        "    if os.path.exists(INPUT_CSV):\n",
        "        print(f\"Reading Place IDs from {INPUT_CSV}...\")\n",
        "        try:\n",
        "            with open(INPUT_CSV, mode='r', encoding='utf-8-sig') as f:\n",
        "                reader = csv.DictReader(f)\n",
        "                for row in reader:\n",
        "                    if 'place_id' in row:\n",
        "                        place_ids.append(row['place_id'].strip())\n",
        "                    else:\n",
        "                        place_ids.append(list(row.values())[0].strip())\n",
        "        except Exception:\n",
        "            place_ids = DEFAULT_PLACE_IDS\n",
        "    else:\n",
        "        place_ids = DEFAULT_PLACE_IDS\n",
        "\n",
        "    place_ids = [pid for pid in place_ids if pid]\n",
        "    print(f\"Processing {len(place_ids)} Place IDs...\")\n",
        "\n",
        "    # 2. Load Existing Signatures (To prevent duplicates)\n",
        "    print(\"Loading existing records to check for duplicates...\")\n",
        "    existing_signatures = get_existing_signatures(OUTPUT_FILE)\n",
        "    print(f\"Found {len(existing_signatures)} existing reviews in database.\")\n",
        "\n",
        "    # 3. Prepare Output\n",
        "    headers = [\n",
        "        \"run_date\",\n",
        "        \"place_id\",\n",
        "        \"place_name\",\n",
        "        \"city\",\n",
        "        \"overall_rating\",\n",
        "        \"total_ratings\",\n",
        "        \"business_status\",\n",
        "        \"author_name\",\n",
        "        \"rating\",\n",
        "        \"text\",\n",
        "        \"review_date\"\n",
        "    ]\n",
        "\n",
        "    file_exists = os.path.isfile(OUTPUT_FILE)\n",
        "\n",
        "    # 4. Open File\n",
        "    with open(OUTPUT_FILE, mode='a', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        if not file_exists:\n",
        "            writer.writerow(headers)\n",
        "\n",
        "        new_reviews_count = 0\n",
        "\n",
        "        for pid in place_ids:\n",
        "            print(f\"Fetching data for: {pid}\")\n",
        "\n",
        "            details = get_place_details(pid)\n",
        "\n",
        "            if details:\n",
        "                base_row = [\n",
        "                    current_run_date,\n",
        "                    details[\"place_id\"],\n",
        "                    details[\"place_name\"],\n",
        "                    details[\"city\"],\n",
        "                    details[\"overall_rating\"],\n",
        "                    details[\"total_ratings\"],\n",
        "                    details[\"business_status\"]\n",
        "                ]\n",
        "\n",
        "                reviews_list = details[\"reviews\"]\n",
        "\n",
        "                if reviews_list:\n",
        "                    # Check every review against existing signatures\n",
        "                    for r in reviews_list:\n",
        "                        r_author = r.get(\"author_name\", \"\").strip()\n",
        "                        r_rating = str(r.get(\"rating\", \"\")).strip() # Ensure string for comparison\n",
        "                        r_text = r.get(\"text\", \"\").strip()\n",
        "                        r_date = format_date(r.get(\"time\")).strip()\n",
        "\n",
        "                        # Create fingerprint for THIS new review\n",
        "                        current_signature = (pid, r_author, r_rating, r_text, r_date)\n",
        "\n",
        "                        if current_signature in existing_signatures:\n",
        "                            # SKIP - Already exists\n",
        "                            continue\n",
        "\n",
        "                        # WRITE - New review\n",
        "                        full_row = base_row + [r_author, r_rating, r_text, r_date]\n",
        "                        writer.writerow(full_row)\n",
        "\n",
        "                        # Add to signatures so we don't add it again if it appears twice in one run (unlikely but safe)\n",
        "                        existing_signatures.add(current_signature)\n",
        "                        new_reviews_count += 1\n",
        "                else:\n",
        "                    # If NO reviews exist for the place, we usually want to log that the check happened.\n",
        "                    # We treat this as a status update.\n",
        "                    full_row = base_row + [\"\", \"\", \"\", \"\"]\n",
        "                    writer.writerow(full_row)\n",
        "\n",
        "            else:\n",
        "                # Error row\n",
        "                error_row = [current_run_date, pid] + [\"\"] * 9\n",
        "                writer.writerow(error_row)\n",
        "\n",
        "            time.sleep(0.2)\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"Done. Added {new_reviews_count} new reviews to {OUTPUT_FILE}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hX7hq2aEEgxQ",
        "outputId": "47ffb7f4-56ea-45b2-83b4-d2bb5b8343b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 2 Place IDs...\n",
            "Loading existing records to check for duplicates...\n",
            "Found 10 existing reviews in database.\n",
            "Fetching data for: ChIJZU8k2vU9rjsRK9gXANkis7k\n",
            "Fetching data for: ChIJK7aICxoVrjsRnjZbF0TjvNo\n",
            "------------------------------\n",
            "Done. Added 0 new reviews to places_reviews_long_format.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "import time\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "# ================= CONFIGURATION =================\n",
        "API_KEY = \"AIzaSyDlKfGNRGtlfBKe9X_lktXeM79pYPMiO0E\"  # <--- REPLACE THIS\n",
        "\n",
        "# INPUT: The file containing City (Col 1) and Place ID (Col 2)\n",
        "INPUT_CSV_PATH = \"/content/places_reviews_long_format.csv\"\n",
        "\n",
        "# OUTPUT: Where the full review data will be saved\n",
        "OUTPUT_CSV_PATH = \"/content/final_reviews_output.csv\"\n",
        "\n",
        "BASE_URL = \"https://maps.googleapis.com/maps/api/place/details/json\"\n",
        "\n",
        "# ================= LOGIC =================\n",
        "\n",
        "def get_city_from_components(components):\n",
        "    if not components:\n",
        "        return \"\"\n",
        "    for comp in components:\n",
        "        if \"locality\" in comp.get(\"types\", []):\n",
        "            return comp.get(\"long_name\", \"\")\n",
        "    for comp in components:\n",
        "        if \"administrative_area_level_2\" in comp.get(\"types\", []):\n",
        "            return comp.get(\"long_name\", \"\")\n",
        "    return \"\"\n",
        "\n",
        "def format_date(timestamp):\n",
        "    if not timestamp:\n",
        "        return \"\"\n",
        "    try:\n",
        "        return datetime.datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d')\n",
        "    except (ValueError, TypeError):\n",
        "        return \"\"\n",
        "\n",
        "def get_existing_signatures(filename):\n",
        "    \"\"\"\n",
        "    Reads the OUTPUT CSV to check for duplicates.\n",
        "    Signature = (place_id, author_name, rating, text, review_date)\n",
        "    \"\"\"\n",
        "    signatures = set()\n",
        "    if not os.path.isfile(filename):\n",
        "        return signatures\n",
        "\n",
        "    try:\n",
        "        with open(filename, mode='r', encoding='utf-8') as f:\n",
        "            reader = csv.DictReader(f)\n",
        "            for row in reader:\n",
        "                sig = (\n",
        "                    row.get(\"place_id\", \"\").strip(),\n",
        "                    row.get(\"author_name\", \"\").strip(),\n",
        "                    row.get(\"rating\", \"\").strip(),\n",
        "                    row.get(\"text\", \"\").strip(),\n",
        "                    row.get(\"review_date\", \"\").strip()\n",
        "                )\n",
        "                signatures.add(sig)\n",
        "    except Exception as e:\n",
        "        print(f\"Note: Output file exists but could not be read for deduplication: {e}\")\n",
        "\n",
        "    return signatures\n",
        "\n",
        "def get_place_details(place_id):\n",
        "    params = {\n",
        "        \"place_id\": place_id,\n",
        "        \"key\": API_KEY,\n",
        "        \"reviews_sort\": \"newest\",\n",
        "        \"language\": \"en\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(BASE_URL, params=params)\n",
        "        data = response.json()\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] HTTP Request failed for {place_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "    status = data.get(\"status\")\n",
        "    if status != \"OK\":\n",
        "        print(f\"[API ERROR] Place ID {place_id} returned status: {status}\")\n",
        "        return None\n",
        "\n",
        "    result = data.get(\"result\", {})\n",
        "\n",
        "    extracted_data = {\n",
        "        \"place_id\": place_id,\n",
        "        \"place_name\": result.get(\"name\", \"\"),\n",
        "        \"city\": get_city_from_components(result.get(\"address_components\", [])),\n",
        "        \"overall_rating\": result.get(\"rating\", \"\"),\n",
        "        \"total_ratings\": result.get(\"user_ratings_total\", \"\"),\n",
        "        \"business_status\": result.get(\"business_status\", \"\"),\n",
        "        \"reviews\": []\n",
        "    }\n",
        "\n",
        "    raw_reviews = result.get(\"reviews\", [])\n",
        "    if raw_reviews:\n",
        "        sorted_reviews = sorted(raw_reviews, key=lambda x: x.get(\"time\", 0), reverse=True)\n",
        "        extracted_data[\"reviews\"] = sorted_reviews[:5]\n",
        "\n",
        "    return extracted_data\n",
        "\n",
        "def main():\n",
        "    current_run_date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "    # 1. Load IDs from the specified input path\n",
        "    place_ids = []\n",
        "    if os.path.exists(INPUT_CSV_PATH):\n",
        "        print(f\"Reading Place IDs from {INPUT_CSV_PATH}...\")\n",
        "        try:\n",
        "            with open(INPUT_CSV_PATH, mode='r', encoding='utf-8-sig') as f:\n",
        "                # We use csv.reader to access columns by Index\n",
        "                reader = csv.reader(f)\n",
        "\n",
        "                # Skip the header row\n",
        "                next(reader, None)\n",
        "\n",
        "                for row in reader:\n",
        "                    # Ensure row has at least 2 columns\n",
        "                    if len(row) >= 2:\n",
        "                        # Column 0 is City, Column 1 is Place ID\n",
        "                        pid = row[1].strip()\n",
        "                        if pid:\n",
        "                            place_ids.append(pid)\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading input CSV: {e}\")\n",
        "            return\n",
        "    else:\n",
        "        print(f\"ERROR: Input file not found at {INPUT_CSV_PATH}\")\n",
        "        return\n",
        "\n",
        "    # Remove any empty entries\n",
        "    place_ids = [pid for pid in place_ids if pid]\n",
        "    print(f\"Found {len(place_ids)} Place IDs to process.\")\n",
        "\n",
        "    # 2. Load Existing Signatures from the OUTPUT file (to prevent duplicates)\n",
        "    existing_signatures = get_existing_signatures(OUTPUT_CSV_PATH)\n",
        "    print(f\"Found {len(existing_signatures)} existing reviews in output file.\")\n",
        "\n",
        "    # 3. Prepare Output Headers\n",
        "    headers = [\n",
        "        \"run_date\",\n",
        "        \"place_id\",\n",
        "        \"place_name\",\n",
        "        \"city\",\n",
        "        \"overall_rating\",\n",
        "        \"total_ratings\",\n",
        "        \"business_status\",\n",
        "        \"author_name\",\n",
        "        \"rating\",\n",
        "        \"text\",\n",
        "        \"review_date\"\n",
        "    ]\n",
        "\n",
        "    file_exists = os.path.isfile(OUTPUT_CSV_PATH)\n",
        "\n",
        "    # 4. Open Output File (Append Mode)\n",
        "    print(f\"Writing data to {OUTPUT_CSV_PATH}...\")\n",
        "    with open(OUTPUT_CSV_PATH, mode='a', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "\n",
        "        # Write headers only if file is new\n",
        "        if not file_exists:\n",
        "            writer.writerow(headers)\n",
        "\n",
        "        new_reviews_count = 0\n",
        "\n",
        "        for pid in place_ids:\n",
        "            print(f\"Fetching data for: {pid}\")\n",
        "\n",
        "            details = get_place_details(pid)\n",
        "\n",
        "            if details:\n",
        "                base_row = [\n",
        "                    current_run_date,\n",
        "                    details[\"place_id\"],\n",
        "                    details[\"place_name\"],\n",
        "                    details[\"city\"],\n",
        "                    details[\"overall_rating\"],\n",
        "                    details[\"total_ratings\"],\n",
        "                    details[\"business_status\"]\n",
        "                ]\n",
        "\n",
        "                reviews_list = details[\"reviews\"]\n",
        "\n",
        "                if reviews_list:\n",
        "                    for r in reviews_list:\n",
        "                        r_author = r.get(\"author_name\", \"\").strip()\n",
        "                        r_rating = str(r.get(\"rating\", \"\")).strip()\n",
        "                        r_text = r.get(\"text\", \"\").strip()\n",
        "                        r_date = format_date(r.get(\"time\")).strip()\n",
        "\n",
        "                        # Duplicate Check\n",
        "                        current_signature = (pid, r_author, r_rating, r_text, r_date)\n",
        "\n",
        "                        if current_signature in existing_signatures:\n",
        "                            continue\n",
        "\n",
        "                        # Write new unique review\n",
        "                        full_row = base_row + [r_author, r_rating, r_text, r_date]\n",
        "                        writer.writerow(full_row)\n",
        "\n",
        "                        existing_signatures.add(current_signature)\n",
        "                        new_reviews_count += 1\n",
        "                else:\n",
        "                    # Write one row if no reviews exist\n",
        "                    full_row = base_row + [\"\", \"\", \"\", \"\"]\n",
        "                    writer.writerow(full_row)\n",
        "\n",
        "            else:\n",
        "                # API Error\n",
        "                error_row = [current_run_date, pid] + [\"\"] * 9\n",
        "                writer.writerow(error_row)\n",
        "\n",
        "            time.sleep(0.2) # Rate limit\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"Done. Added {new_reviews_count} new reviews to {OUTPUT_CSV_PATH}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOIfpHqJGodh",
        "outputId": "9ef0a3db-1c46-42bd-d881-f15846e0b20e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading Place IDs from /content/places_reviews_long_format.csv...\n",
            "Found 10 Place IDs to process.\n",
            "Found 10 existing reviews in output file.\n",
            "Writing data to /content/final_reviews_output.csv...\n",
            "Fetching data for: ChIJK7aICxoVrjsRnjZbF0TjvNo\n",
            "Fetching data for: ChIJK7aICxoVrjsRnjZbF0TjvNo\n",
            "Fetching data for: ChIJK7aICxoVrjsRnjZbF0TjvNo\n",
            "Fetching data for: ChIJK7aICxoVrjsRnjZbF0TjvNo\n",
            "Fetching data for: ChIJK7aICxoVrjsRnjZbF0TjvNo\n",
            "Fetching data for: ChIJZU8k2vU9rjsRK9gXANkis7k\n",
            "Fetching data for: ChIJZU8k2vU9rjsRK9gXANkis7k\n",
            "Fetching data for: ChIJZU8k2vU9rjsRK9gXANkis7k\n",
            "Fetching data for: ChIJZU8k2vU9rjsRK9gXANkis7k\n",
            "Fetching data for: ChIJZU8k2vU9rjsRK9gXANkis7k\n",
            "------------------------------\n",
            "Done. Added 0 new reviews to /content/final_reviews_output.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "import time\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "# ================= CONFIGURATION =================\n",
        "API_KEY = \"AIzaSyDlKfGNRGtlfBKe9X_lktXeM79pYPMiO0E\"  # <--- REPLACE THIS\n",
        "\n",
        "# INPUT: The file containing City (Col 1) and Place ID (Col 2)\n",
        "INPUT_CSV_PATH = \"/content/places_reviews_long_format.csv\"\n",
        "\n",
        "# OUTPUT: Where the full review data will be saved\n",
        "OUTPUT_CSV_PATH = \"/content/final_reviews_output.csv\"\n",
        "\n",
        "BASE_URL = \"https://maps.googleapis.com/maps/api/place/details/json\"\n",
        "\n",
        "# ================= LOGIC =================\n",
        "\n",
        "def get_city_from_components(components):\n",
        "    if not components:\n",
        "        return \"\"\n",
        "    for comp in components:\n",
        "        if \"locality\" in comp.get(\"types\", []):\n",
        "            return comp.get(\"long_name\", \"\")\n",
        "    for comp in components:\n",
        "        if \"administrative_area_level_2\" in comp.get(\"types\", []):\n",
        "            return comp.get(\"long_name\", \"\")\n",
        "    return \"\"\n",
        "\n",
        "def format_date(timestamp):\n",
        "    if not timestamp:\n",
        "        return \"\"\n",
        "    try:\n",
        "        return datetime.datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d')\n",
        "    except (ValueError, TypeError):\n",
        "        return \"\"\n",
        "\n",
        "def get_existing_signatures(filename):\n",
        "    \"\"\"\n",
        "    Reads the OUTPUT CSV to check for duplicates.\n",
        "    Signature = (place_id, author_name, rating, text, review_date)\n",
        "    \"\"\"\n",
        "    signatures = set()\n",
        "    if not os.path.isfile(filename):\n",
        "        return signatures\n",
        "\n",
        "    try:\n",
        "        with open(filename, mode='r', encoding='utf-8') as f:\n",
        "            reader = csv.DictReader(f)\n",
        "            for row in reader:\n",
        "                sig = (\n",
        "                    row.get(\"place_id\", \"\").strip(),\n",
        "                    row.get(\"author_name\", \"\").strip(),\n",
        "                    row.get(\"rating\", \"\").strip(),\n",
        "                    row.get(\"text\", \"\").strip(),\n",
        "                    row.get(\"review_date\", \"\").strip()\n",
        "                )\n",
        "                signatures.add(sig)\n",
        "    except Exception as e:\n",
        "        print(f\"Note: Output file exists but could not be read for deduplication: {e}\")\n",
        "\n",
        "    return signatures\n",
        "\n",
        "def get_place_details(place_id):\n",
        "    params = {\n",
        "        \"place_id\": place_id,\n",
        "        \"key\": API_KEY,\n",
        "        \"reviews_sort\": \"newest\",\n",
        "        \"language\": \"en\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(BASE_URL, params=params)\n",
        "        data = response.json()\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] HTTP Request failed for {place_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "    status = data.get(\"status\")\n",
        "    if status != \"OK\":\n",
        "        print(f\"[API ERROR] Place ID {place_id} returned status: {status}\")\n",
        "        return None\n",
        "\n",
        "    result = data.get(\"result\", {})\n",
        "\n",
        "    extracted_data = {\n",
        "        \"place_id\": place_id,\n",
        "        \"place_name\": result.get(\"name\", \"\"),\n",
        "        \"city\": get_city_from_components(result.get(\"address_components\", [])),\n",
        "        \"overall_rating\": result.get(\"rating\", \"\"),\n",
        "        \"total_ratings\": result.get(\"user_ratings_total\", \"\"),\n",
        "        \"business_status\": result.get(\"business_status\", \"\"),\n",
        "        \"reviews\": []\n",
        "    }\n",
        "\n",
        "    raw_reviews = result.get(\"reviews\", [])\n",
        "    if raw_reviews:\n",
        "        sorted_reviews = sorted(raw_reviews, key=lambda x: x.get(\"time\", 0), reverse=True)\n",
        "        extracted_data[\"reviews\"] = sorted_reviews[:5]\n",
        "\n",
        "    return extracted_data\n",
        "\n",
        "def main():\n",
        "    current_run_date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "    # 1. Load IDs from the specified input path\n",
        "    # We use a set to ensure we don't process duplicates, but we count all rows first\n",
        "    place_ids = []\n",
        "\n",
        "    if os.path.exists(INPUT_CSV_PATH):\n",
        "        print(f\"Reading Place IDs from {INPUT_CSV_PATH}...\")\n",
        "        try:\n",
        "            with open(INPUT_CSV_PATH, mode='r', encoding='utf-8-sig') as f:\n",
        "                reader = csv.reader(f)\n",
        "\n",
        "                # Read all rows into a list to count them accurately\n",
        "                all_rows = list(reader)\n",
        "\n",
        "                # Calculate total data rows (Total rows - 1 for header)\n",
        "                total_rows_in_file = len(all_rows)\n",
        "                data_rows_count = max(0, total_rows_in_file - 1)\n",
        "\n",
        "                print(f\"Total rows in input file: {total_rows_in_file}\")\n",
        "                print(f\"Data rows (excluding header): {data_rows_count}\")\n",
        "\n",
        "                # Iterate starting from index 1 (skipping index 0 which is header)\n",
        "                for i in range(1, len(all_rows)):\n",
        "                    row = all_rows[i]\n",
        "                    # Ensure row has at least 2 columns (City, PlaceID)\n",
        "                    if len(row) >= 2:\n",
        "                        pid = row[1].strip()\n",
        "                        if pid:\n",
        "                            place_ids.append(pid)\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading input CSV: {e}\")\n",
        "            return\n",
        "    else:\n",
        "        print(f\"ERROR: Input file not found at {INPUT_CSV_PATH}\")\n",
        "        return\n",
        "\n",
        "    # Deduplicate list to save API calls (in case input has same ID twice)\n",
        "    # If you explicitly want to run duplicates, remove the set() conversion.\n",
        "    unique_place_ids = list(set(place_ids))\n",
        "\n",
        "    print(f\"--------------------------------------------------\")\n",
        "    print(f\"Unique Place IDs extracted: {len(unique_place_ids)}\")\n",
        "    print(f\"--------------------------------------------------\")\n",
        "\n",
        "    if len(unique_place_ids) == 0:\n",
        "        print(\"No Place IDs found. Check your input CSV format.\")\n",
        "        return\n",
        "\n",
        "    # 2. Load Existing Signatures (Prevent Duplicates in Output)\n",
        "    existing_signatures = get_existing_signatures(OUTPUT_CSV_PATH)\n",
        "\n",
        "    # 3. Prepare Output\n",
        "    headers = [\n",
        "        \"run_date\", \"place_id\", \"place_name\", \"city\", \"overall_rating\",\n",
        "        \"total_ratings\", \"business_status\", \"author_name\", \"rating\", \"text\", \"review_date\"\n",
        "    ]\n",
        "\n",
        "    file_exists = os.path.isfile(OUTPUT_CSV_PATH)\n",
        "\n",
        "    print(f\"Writing data to {OUTPUT_CSV_PATH}...\")\n",
        "\n",
        "    # 4. Open Output File\n",
        "    with open(OUTPUT_CSV_PATH, mode='a', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "\n",
        "        if not file_exists:\n",
        "            writer.writerow(headers)\n",
        "\n",
        "        processed_count = 0\n",
        "        new_reviews_count = 0\n",
        "\n",
        "        for pid in unique_place_ids:\n",
        "            processed_count += 1\n",
        "            print(f\"[{processed_count}/{len(unique_place_ids)}] Fetching data for: {pid}\")\n",
        "\n",
        "            details = get_place_details(pid)\n",
        "\n",
        "            if details:\n",
        "                base_row = [\n",
        "                    current_run_date,\n",
        "                    details[\"place_id\"],\n",
        "                    details[\"place_name\"],\n",
        "                    details[\"city\"],\n",
        "                    details[\"overall_rating\"],\n",
        "                    details[\"total_ratings\"],\n",
        "                    details[\"business_status\"]\n",
        "                ]\n",
        "\n",
        "                reviews_list = details[\"reviews\"]\n",
        "\n",
        "                if reviews_list:\n",
        "                    for r in reviews_list:\n",
        "                        r_author = r.get(\"author_name\", \"\").strip()\n",
        "                        r_rating = str(r.get(\"rating\", \"\")).strip()\n",
        "                        r_text = r.get(\"text\", \"\").strip()\n",
        "                        r_date = format_date(r.get(\"time\")).strip()\n",
        "\n",
        "                        # Duplicate Check\n",
        "                        current_signature = (pid, r_author, r_rating, r_text, r_date)\n",
        "\n",
        "                        if current_signature in existing_signatures:\n",
        "                            continue\n",
        "\n",
        "                        full_row = base_row + [r_author, r_rating, r_text, r_date]\n",
        "                        writer.writerow(full_row)\n",
        "                        existing_signatures.add(current_signature)\n",
        "                        new_reviews_count += 1\n",
        "                else:\n",
        "                    full_row = base_row + [\"\", \"\", \"\", \"\"]\n",
        "                    writer.writerow(full_row)\n",
        "            else:\n",
        "                # API Error\n",
        "                error_row = [current_run_date, pid] + [\"\"] * 9\n",
        "                writer.writerow(error_row)\n",
        "\n",
        "            time.sleep(0.2)\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"Done. Processed {processed_count} IDs.\")\n",
        "    print(f\"Added {new_reviews_count} new reviews to {OUTPUT_CSV_PATH}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTGWxGI5HRDY",
        "outputId": "8241d071-9d92-407d-eb95-51348ab3aaba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: Input file not found at /content/places_reviews_long_format.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "import time\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "# ================= CONFIGURATION =================\n",
        "API_KEY = \"AIzaSyDlKfGNRGtlfBKe9X_lktXeM79pYPMiO0E\"  # <--- REPLACE THIS\n",
        "\n",
        "# UPDATED INPUT PATH based on your request\n",
        "INPUT_CSV_PATH = \"/content/barbeque_nation_master_place_ids.csv\"\n",
        "\n",
        "# OUTPUT: Where the results will be saved\n",
        "OUTPUT_CSV_PATH = \"/content/final_reviews_output.csv\"\n",
        "\n",
        "# FALLBACK LIST: Used only if the Input CSV is not found\n",
        "DEFAULT_PLACE_IDS = [\n",
        "    \"ChIJmaVf3D3J5zsRunDxXYOoqnM\",\n",
        "]\n",
        "\n",
        "BASE_URL = \"https://maps.googleapis.com/maps/api/place/details/json\"\n",
        "\n",
        "# ================= LOGIC =================\n",
        "\n",
        "def get_city_from_components(components):\n",
        "    if not components:\n",
        "        return \"\"\n",
        "    for comp in components:\n",
        "        if \"locality\" in comp.get(\"types\", []):\n",
        "            return comp.get(\"long_name\", \"\")\n",
        "    for comp in components:\n",
        "        if \"administrative_area_level_2\" in comp.get(\"types\", []):\n",
        "            return comp.get(\"long_name\", \"\")\n",
        "    return \"\"\n",
        "\n",
        "def format_date(timestamp):\n",
        "    if not timestamp:\n",
        "        return \"\"\n",
        "    try:\n",
        "        return datetime.datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d')\n",
        "    except (ValueError, TypeError):\n",
        "        return \"\"\n",
        "\n",
        "def get_existing_signatures(filename):\n",
        "    \"\"\"\n",
        "    Reads the OUTPUT CSV to check for duplicates.\n",
        "    Signature = (place_id, author_name, rating, text, review_date)\n",
        "    \"\"\"\n",
        "    signatures = set()\n",
        "    if not os.path.isfile(filename):\n",
        "        return signatures\n",
        "    try:\n",
        "        with open(filename, mode='r', encoding='utf-8') as f:\n",
        "            reader = csv.DictReader(f)\n",
        "            for row in reader:\n",
        "                sig = (\n",
        "                    row.get(\"place_id\", \"\").strip(),\n",
        "                    row.get(\"author_name\", \"\").strip(),\n",
        "                    row.get(\"rating\", \"\").strip(),\n",
        "                    row.get(\"text\", \"\").strip(),\n",
        "                    row.get(\"review_date\", \"\").strip()\n",
        "                )\n",
        "                signatures.add(sig)\n",
        "    except Exception as e:\n",
        "        print(f\"Note: Output file exists but could not be read: {e}\")\n",
        "    return signatures\n",
        "\n",
        "def get_place_details(place_id):\n",
        "    params = {\n",
        "        \"place_id\": place_id,\n",
        "        \"key\": API_KEY,\n",
        "        \"reviews_sort\": \"newest\",\n",
        "        \"language\": \"en\"\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(BASE_URL, params=params)\n",
        "        data = response.json()\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] HTTP Request failed for {place_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "    if data.get(\"status\") != \"OK\":\n",
        "        print(f\"[API ERROR] Place ID {place_id} returned status: {data.get('status')}\")\n",
        "        return None\n",
        "\n",
        "    result = data.get(\"result\", {})\n",
        "    extracted_data = {\n",
        "        \"place_id\": place_id,\n",
        "        \"place_name\": result.get(\"name\", \"\"),\n",
        "        \"city\": get_city_from_components(result.get(\"address_components\", [])),\n",
        "        \"overall_rating\": result.get(\"rating\", \"\"),\n",
        "        \"total_ratings\": result.get(\"user_ratings_total\", \"\"),\n",
        "        \"business_status\": result.get(\"business_status\", \"\"),\n",
        "        \"reviews\": []\n",
        "    }\n",
        "\n",
        "    raw_reviews = result.get(\"reviews\", [])\n",
        "    if raw_reviews:\n",
        "        sorted_reviews = sorted(raw_reviews, key=lambda x: x.get(\"time\", 0), reverse=True)\n",
        "        extracted_data[\"reviews\"] = sorted_reviews[:5]\n",
        "\n",
        "    return extracted_data\n",
        "\n",
        "def main():\n",
        "    current_run_date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "    # 1. READ INPUT IDs\n",
        "    place_ids = []\n",
        "\n",
        "    # Check if the INPUT file exists\n",
        "    if os.path.exists(INPUT_CSV_PATH):\n",
        "        print(f\"Reading Place IDs from {INPUT_CSV_PATH}...\")\n",
        "        try:\n",
        "            with open(INPUT_CSV_PATH, mode='r', encoding='utf-8-sig') as f:\n",
        "                reader = csv.reader(f)\n",
        "                all_rows = list(reader)\n",
        "\n",
        "                total_rows = len(all_rows)\n",
        "                print(f\"Total rows in input file: {total_rows}\")\n",
        "\n",
        "                # Loop (Skip header at index 0)\n",
        "                for i in range(1, len(all_rows)):\n",
        "                    row = all_rows[i]\n",
        "                    # Ensure row has at least 2 columns (City, PlaceID)\n",
        "                    if len(row) >= 2:\n",
        "                        pid = row[1].strip()\n",
        "                        if pid:\n",
        "                            place_ids.append(pid)\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading CSV: {e}. Switching to default list.\")\n",
        "            place_ids = DEFAULT_PLACE_IDS\n",
        "    else:\n",
        "        # FALLBACK\n",
        "        print(f\"WARNING: Input file not found at {INPUT_CSV_PATH}\")\n",
        "        print(\"Please upload 'barbeque_nation_master_place_ids.csv' to the content folder.\")\n",
        "        place_ids = DEFAULT_PLACE_IDS\n",
        "\n",
        "    # Deduplicate IDs\n",
        "    unique_place_ids = list(set(place_ids))\n",
        "    print(f\"Processing {len(unique_place_ids)} unique Place IDs.\")\n",
        "\n",
        "    # 2. LOAD EXISTING DATA (To check for duplicates)\n",
        "    existing_signatures = get_existing_signatures(OUTPUT_CSV_PATH)\n",
        "\n",
        "    # 3. WRITE OUTPUT\n",
        "    headers = [\n",
        "        \"run_date\", \"place_id\", \"place_name\", \"city\", \"overall_rating\",\n",
        "        \"total_ratings\", \"business_status\", \"author_name\", \"rating\", \"text\", \"review_date\"\n",
        "    ]\n",
        "\n",
        "    file_exists = os.path.isfile(OUTPUT_CSV_PATH)\n",
        "\n",
        "    # Open with 'a' (Append) mode\n",
        "    with open(OUTPUT_CSV_PATH, mode='a', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "\n",
        "        if not file_exists:\n",
        "            writer.writerow(headers)\n",
        "            print(f\"Created new file: {OUTPUT_CSV_PATH}\")\n",
        "        else:\n",
        "            print(f\"Appending to existing file: {OUTPUT_CSV_PATH}\")\n",
        "\n",
        "        processed_count = 0\n",
        "        new_reviews_count = 0\n",
        "\n",
        "        for pid in unique_place_ids:\n",
        "            processed_count += 1\n",
        "            print(f\"[{processed_count}/{len(unique_place_ids)}] Fetching: {pid}\")\n",
        "\n",
        "            details = get_place_details(pid)\n",
        "\n",
        "            if details:\n",
        "                base_row = [\n",
        "                    current_run_date, details[\"place_id\"], details[\"place_name\"],\n",
        "                    details[\"city\"], details[\"overall_rating\"], details[\"total_ratings\"],\n",
        "                    details[\"business_status\"]\n",
        "                ]\n",
        "\n",
        "                reviews_list = details[\"reviews\"]\n",
        "                if reviews_list:\n",
        "                    for r in reviews_list:\n",
        "                        r_author = r.get(\"author_name\", \"\").strip()\n",
        "                        r_rating = str(r.get(\"rating\", \"\")).strip()\n",
        "                        r_text = r.get(\"text\", \"\").strip()\n",
        "                        r_date = format_date(r.get(\"time\")).strip()\n",
        "\n",
        "                        # Check Duplicates\n",
        "                        sig = (pid, r_author, r_rating, r_text, r_date)\n",
        "                        if sig in existing_signatures:\n",
        "                            continue\n",
        "\n",
        "                        writer.writerow(base_row + [r_author, r_rating, r_text, r_date])\n",
        "                        existing_signatures.add(sig)\n",
        "                        new_reviews_count += 1\n",
        "                else:\n",
        "                    writer.writerow(base_row + [\"\", \"\", \"\", \"\"])\n",
        "            else:\n",
        "                writer.writerow([current_run_date, pid] + [\"\"] * 9)\n",
        "\n",
        "            time.sleep(0.2)\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"Done. Added {new_reviews_count} new reviews to {OUTPUT_CSV_PATH}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "6UKKx_30I3wI",
        "outputId": "f6e553e4-82a7-477a-9b65-c1fabe5322b3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading Place IDs from /content/barbeque_nation_master_place_ids.csv...\n",
            "Total rows in input file: 220\n",
            "Processing 217 unique Place IDs.\n",
            "Created new file: /content/final_reviews_output.csv\n",
            "[1/217] Fetching: ChIJW_BsWmpmUjoRC9cGfUyljmU\n",
            "[2/217] Fetching: ChIJt6OEQyOf-DkR_Jx3jwHUITg\n",
            "[3/217] Fetching: ChIJ_7BHmcZbUjoReu86baMPVFE\n",
            "[4/217] Fetching: ChIJGwTIIxQ5Xj4Rvb9PiG90yLY\n",
            "[5/217] Fetching: ChIJ6zqdhfVLTToRYLCuyAuAWzw\n",
            "[6/217] Fetching: ChIJY96VoDNaozsRVmSewjbbvKk\n",
            "[7/217] Fetching: ChIJ-8vxhrsHqTsRDHZFiERBZ2s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1918226869.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1918226869.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_run_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "import time\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "# ================= CONFIGURATION =================\n",
        "API_KEY = \"AIzaSyDlKfGNRGtlfBKe9X_lktXeM79pYPMiO0E\"  # <--- REPLACE WITH YOUR ACTUAL KEY\n",
        "\n",
        "# INPUT: The file containing City (Col 0), Place ID (Col 1), Place Name (Col 2)\n",
        "INPUT_CSV_PATH = \"/content/barbeque_nation_master_place_ids.csv\"\n",
        "\n",
        "# OUTPUT: Where the results will be saved\n",
        "OUTPUT_CSV_PATH = \"/content/final_reviews_output.csv\"\n",
        "\n",
        "# FALLBACK LIST: Used only if the Input CSV is not found\n",
        "DEFAULT_PLACE_IDS = [\n",
        "    \"ChIJmaVf3D3J5zsRunDxXYOoqnM\",\n",
        "]\n",
        "\n",
        "BASE_URL = \"https://maps.googleapis.com/maps/api/place/details/json\"\n",
        "\n",
        "# ================= LOGIC =================\n",
        "\n",
        "def get_city_from_components(components):\n",
        "    if not components:\n",
        "        return \"\"\n",
        "    for comp in components:\n",
        "        if \"locality\" in comp.get(\"types\", []):\n",
        "            return comp.get(\"long_name\", \"\")\n",
        "    for comp in components:\n",
        "        if \"administrative_area_level_2\" in comp.get(\"types\", []):\n",
        "            return comp.get(\"long_name\", \"\")\n",
        "    return \"\"\n",
        "\n",
        "def format_date(timestamp):\n",
        "    if not timestamp:\n",
        "        return \"\"\n",
        "    try:\n",
        "        return datetime.datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d')\n",
        "    except (ValueError, TypeError):\n",
        "        return \"\"\n",
        "\n",
        "def get_existing_signatures(filename):\n",
        "    \"\"\"\n",
        "    Reads the OUTPUT CSV to check for duplicates.\n",
        "    Signature = (place_id, author_name, rating, text, review_date)\n",
        "    \"\"\"\n",
        "    signatures = set()\n",
        "    if not os.path.isfile(filename):\n",
        "        return signatures\n",
        "    try:\n",
        "        with open(filename, mode='r', encoding='utf-8') as f:\n",
        "            reader = csv.DictReader(f)\n",
        "            for row in reader:\n",
        "                sig = (\n",
        "                    row.get(\"place_id\", \"\").strip(),\n",
        "                    row.get(\"author_name\", \"\").strip(),\n",
        "                    row.get(\"rating\", \"\").strip(),\n",
        "                    row.get(\"text\", \"\").strip(),\n",
        "                    row.get(\"review_date\", \"\").strip()\n",
        "                )\n",
        "                signatures.add(sig)\n",
        "    except Exception as e:\n",
        "        print(f\"Note: Output file exists but could not be read: {e}\")\n",
        "    return signatures\n",
        "\n",
        "def get_place_details(place_id):\n",
        "    params = {\n",
        "        \"place_id\": place_id,\n",
        "        \"key\": API_KEY,\n",
        "        \"reviews_sort\": \"newest\",\n",
        "        \"language\": \"en\"\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(BASE_URL, params=params)\n",
        "        data = response.json()\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] HTTP Request failed for {place_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "    if data.get(\"status\") != \"OK\":\n",
        "        print(f\"[API ERROR] Place ID {place_id} returned status: {data.get('status')}\")\n",
        "        return None\n",
        "\n",
        "    result = data.get(\"result\", {})\n",
        "    extracted_data = {\n",
        "        \"place_id\": place_id,\n",
        "        \"place_name\": result.get(\"name\", \"\"),\n",
        "        \"city\": get_city_from_components(result.get(\"address_components\", [])),\n",
        "        \"overall_rating\": result.get(\"rating\", \"\"),\n",
        "        \"total_ratings\": result.get(\"user_ratings_total\", \"\"),\n",
        "        \"business_status\": result.get(\"business_status\", \"\"),\n",
        "        \"reviews\": []\n",
        "    }\n",
        "\n",
        "    raw_reviews = result.get(\"reviews\", [])\n",
        "    if raw_reviews:\n",
        "        sorted_reviews = sorted(raw_reviews, key=lambda x: x.get(\"time\", 0), reverse=True)\n",
        "        extracted_data[\"reviews\"] = sorted_reviews[:5]\n",
        "\n",
        "    return extracted_data\n",
        "\n",
        "def main():\n",
        "    current_run_date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "    # 1. READ INPUT IDs\n",
        "    place_ids = []\n",
        "\n",
        "    # Check if the INPUT file exists\n",
        "    if os.path.exists(INPUT_CSV_PATH):\n",
        "        print(f\"Reading Place IDs from {INPUT_CSV_PATH}...\")\n",
        "        try:\n",
        "            with open(INPUT_CSV_PATH, mode='r', encoding='utf-8-sig') as f:\n",
        "                reader = csv.reader(f)\n",
        "                all_rows = list(reader)\n",
        "\n",
        "                total_rows = len(all_rows)\n",
        "                print(f\"Total rows in input file: {total_rows}\")\n",
        "\n",
        "                # Loop (Skip header at index 0)\n",
        "                for i in range(1, len(all_rows)):\n",
        "                    row = all_rows[i]\n",
        "                    # Expected structure: [City, PlaceID, PlaceName]\n",
        "                    # We need index 1 for PlaceID\n",
        "                    if len(row) >= 2:\n",
        "                        pid = row[1].strip()\n",
        "                        if pid:\n",
        "                            place_ids.append(pid)\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading CSV: {e}. Switching to default list.\")\n",
        "            place_ids = DEFAULT_PLACE_IDS\n",
        "    else:\n",
        "        # FALLBACK\n",
        "        print(f\"WARNING: Input file not found at {INPUT_CSV_PATH}\")\n",
        "        print(\"Please ensure the previous step ran successfully.\")\n",
        "        place_ids = DEFAULT_PLACE_IDS\n",
        "\n",
        "    # Deduplicate IDs\n",
        "    unique_place_ids = list(set(place_ids))\n",
        "    print(f\"Processing {len(unique_place_ids)} unique Place IDs.\")\n",
        "\n",
        "    # 2. LOAD EXISTING DATA (To check for duplicates)\n",
        "    existing_signatures = get_existing_signatures(OUTPUT_CSV_PATH)\n",
        "\n",
        "    # 3. WRITE OUTPUT\n",
        "    headers = [\n",
        "        \"run_date\", \"place_id\", \"place_name\", \"city\", \"overall_rating\",\n",
        "        \"total_ratings\", \"business_status\", \"author_name\", \"rating\", \"text\", \"review_date\"\n",
        "    ]\n",
        "\n",
        "    file_exists = os.path.isfile(OUTPUT_CSV_PATH)\n",
        "\n",
        "    # Open with 'a' (Append) mode\n",
        "    with open(OUTPUT_CSV_PATH, mode='a', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "\n",
        "        if not file_exists:\n",
        "            writer.writerow(headers)\n",
        "            print(f\"Created new file: {OUTPUT_CSV_PATH}\")\n",
        "        else:\n",
        "            print(f\"Appending to existing file: {OUTPUT_CSV_PATH}\")\n",
        "\n",
        "        processed_count = 0\n",
        "        new_reviews_count = 0\n",
        "\n",
        "        for pid in unique_place_ids:\n",
        "            processed_count += 1\n",
        "            print(f\"[{processed_count}/{len(unique_place_ids)}] Fetching: {pid}\")\n",
        "\n",
        "            details = get_place_details(pid)\n",
        "\n",
        "            if details:\n",
        "                base_row = [\n",
        "                    current_run_date, details[\"place_id\"], details[\"place_name\"],\n",
        "                    details[\"city\"], details[\"overall_rating\"], details[\"total_ratings\"],\n",
        "                    details[\"business_status\"]\n",
        "                ]\n",
        "\n",
        "                reviews_list = details[\"reviews\"]\n",
        "                if reviews_list:\n",
        "                    for r in reviews_list:\n",
        "                        r_author = r.get(\"author_name\", \"\").strip()\n",
        "                        r_rating = str(r.get(\"rating\", \"\")).strip()\n",
        "                        r_text = r.get(\"text\", \"\").strip()\n",
        "                        r_date = format_date(r.get(\"time\")).strip()\n",
        "\n",
        "                        # Check Duplicates\n",
        "                        sig = (pid, r_author, r_rating, r_text, r_date)\n",
        "                        if sig in existing_signatures:\n",
        "                            continue\n",
        "\n",
        "                        writer.writerow(base_row + [r_author, r_rating, r_text, r_date])\n",
        "                        existing_signatures.add(sig)\n",
        "                        new_reviews_count += 1\n",
        "                else:\n",
        "                    writer.writerow(base_row + [\"\", \"\", \"\", \"\"])\n",
        "            else:\n",
        "                writer.writerow([current_run_date, pid] + [\"\"] * 9)\n",
        "\n",
        "            time.sleep(0.2)\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"Done. Added {new_reviews_count} new reviews to {OUTPUT_CSV_PATH}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYXIrAY8LX5i",
        "outputId": "9b5e4b0f-9efc-4481-d64d-a778ba50eb33"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading Place IDs from /content/barbeque_nation_master_place_ids.csv...\n",
            "Total rows in input file: 220\n",
            "Processing 217 unique Place IDs.\n",
            "Created new file: /content/final_reviews_output.csv\n",
            "[1/217] Fetching: ChIJW_BsWmpmUjoRC9cGfUyljmU\n",
            "[2/217] Fetching: ChIJt6OEQyOf-DkR_Jx3jwHUITg\n",
            "[3/217] Fetching: ChIJ_7BHmcZbUjoReu86baMPVFE\n",
            "[4/217] Fetching: ChIJGwTIIxQ5Xj4Rvb9PiG90yLY\n",
            "[5/217] Fetching: ChIJ6zqdhfVLTToRYLCuyAuAWzw\n",
            "[6/217] Fetching: ChIJY96VoDNaozsRVmSewjbbvKk\n",
            "[7/217] Fetching: ChIJ-8vxhrsHqTsRDHZFiERBZ2s\n",
            "[8/217] Fetching: ChIJFaXqjBUoEDkRSh-GGEC7viU\n",
            "[9/217] Fetching: ChIJAQDA-MMtjjkR38ZxjhKaFKY\n",
            "[10/217] Fetching: ChIJ0VfBfkuTyzsRSMtiRe7sep8\n",
            "[11/217] Fetching: ChIJ2b6QC7gR7TkRnipn4fexnE0\n",
            "[12/217] Fetching: ChIJSZVi8q53AjoRkCXde4TByp0\n",
            "[13/217] Fetching: ChIJCYEOFcyl1jsRgai3ay9bGHU\n",
            "[14/217] Fetching: ChIJZ6irvSYVrjsRxp5IyfGEhvY\n",
            "[15/217] Fetching: ChIJq-n4NHhxrzsRqdq6rnaHmNA\n",
            "[16/217] Fetching: ChIJwYmGDBoVrjsRj8ADSu3ctpY\n",
            "[17/217] Fetching: ChIJxedo9LwJGToR7ELyMgJ43Uw\n",
            "[18/217] Fetching: ChIJ8z9ftnCh-DkRSyrpp72IFh8\n",
            "[19/217] Fetching: ChIJLeHfzyDXuDsR3se64ZoBdL4\n",
            "[20/217] Fetching: ChIJ9VGYQdQpCTkRECfJYKfaTbA\n",
            "[21/217] Fetching: ChIJw4uuTdkpODoRnr7HvUAszUk\n",
            "[22/217] Fetching: ChIJTRYvngsXrjsR19TZa3VhYd0\n",
            "[23/217] Fetching: ChIJjb_nG7VrXz4RC2ne2hVE5bA\n",
            "[24/217] Fetching: ChIJ0-n03kaDHjkRyjF62W0mKsU\n",
            "[25/217] Fetching: ChIJD0bLspBdUjoRQRyqApSAeh4\n",
            "[26/217] Fetching: ChIJsSPFhwtZqDsRClmxH55kDXE\n",
            "[27/217] Fetching: ChIJxc5yBACTyzsR61Ldj_HoqW4\n",
            "[28/217] Fetching: ChIJq6qqqq8EDTkRyAJbLbc4Q58\n",
            "[29/217] Fetching: ChIJYd1G9pWbyzsRpNpnXQ3793o\n",
            "[30/217] Fetching: ChIJU_1Dqr1z9zkRtEubBng5E08\n",
            "[31/217] Fetching: ChIJlUibjanlZzkRpzG-9TFSlRs\n",
            "[32/217] Fetching: ChIJGXNqPCg9rjsRhinoy6hXEjs\n",
            "[33/217] Fetching: ChIJXd8xd3a7BTsRb-fISHrcGME\n",
            "[34/217] Fetching: ChIJdx-TwWA5nDkRZHlLLGA9GCE\n",
            "[35/217] Fetching: ChIJW5gW81dmUjoRHUEFvoSLbyE\n",
            "[36/217] Fetching: ChIJIXBLEC3jDDkRurLAi15E2wQ\n",
            "[37/217] Fetching: ChIJ0actGZO7wjsRvcpGc8dcBAs\n",
            "[38/217] Fetching: ChIJU_7sJmfuDzkRYyNCOTDuwQ4\n",
            "[39/217] Fetching: ChIJHfymdLcLKDoRZQMLoNDvyDY\n",
            "[40/217] Fetching: ChIJ7YPZ6oJZWjcRb5nYTnJ4Wsw\n",
            "[41/217] Fetching: ChIJW2723XYbEDkRX3UUoe3ovis\n",
            "[42/217] Fetching: ChIJW4lk0EiBXjkRnR58FgGMsXk\n",
            "[43/217] Fetching: ChIJMaiCX7-XyzsREyrYKz6-Mho\n",
            "[44/217] Fetching: ChIJPUsqQQbrwjsRT8PdI4WxnYU\n",
            "[45/217] Fetching: ChIJVzxsgFvJ5zsRFgkHhiyafrw\n",
            "[46/217] Fetching: ChIJT3vKm109DTkRc__6y7HLF3Y\n",
            "[47/217] Fetching: ChIJIxUu-EURrjsRxKpt9f78SvM\n",
            "[48/217] Fetching: ChIJr3V5uDxmXj4RhWRZLkT87j4\n",
            "[49/217] Fetching: ChIJ5_j8r8B3AjoRDxzzCSwdZGI\n",
            "[50/217] Fetching: ChIJJ9bBAZFB5DkRCiV6t5indE4\n",
            "[51/217] Fetching: ChIJt3TsmjeXyzsRW78eukz3few\n",
            "[52/217] Fetching: ChIJTduybEsTrjsRI7bscLUZx5o\n",
            "[53/217] Fetching: ChIJa7nEjkiRyzsRZDxlC0A6IKY\n",
            "[54/217] Fetching: ChIJrQbM-5J6rzsRtaMQjALd5MI\n",
            "[55/217] Fetching: ChIJQ8_H57vxDDkRC-Urkw88wz4\n",
            "[56/217] Fetching: ChIJq_2o272lST4RSQuwwlyediM\n",
            "[57/217] Fetching: ChIJ-W6xp9UdDTkRCi0FB8rW5KU\n",
            "[58/217] Fetching: ChIJTaPb6esTrjsR7oWu5bRlclE\n",
            "[59/217] Fetching: ChIJA3pQlU6d-DkRpIxVyHATSlo\n",
            "[60/217] Fetching: ChIJb0SxqVcpCTkR2iD1mOyjMTU\n",
            "[61/217] Fetching: ChIJTUsagCf_kT4RG4WOgrecHKQ\n",
            "[62/217] Fetching: ChIJJ4D6dtYAoDkRVtGcNrHTda8\n",
            "[63/217] Fetching: ChIJ17msCgMVrjsRuQFM4oFlvek\n",
            "[64/217] Fetching: ChIJe7vUriepGToRjJAsYcbVOW4\n",
            "[65/217] Fetching: ChIJedogrCHtDzkRBEx9LIGhVr4\n",
            "[66/217] Fetching: ChIJ0xEsj7krCTkRktv4pO4yd3U\n",
            "[67/217] Fetching: ChIJh3NlKuUVrjsRKDVJGhSz9U4\n",
            "[68/217] Fetching: ChIJAQBArTv9mzkRAfGAlUzHEVw\n",
            "[69/217] Fetching: ChIJY_Exh-lZXz4R8lIKPrCE1cQ\n",
            "[70/217] Fetching: ChIJ3TScUevAvzsRytzOAL1tC4U\n",
            "[71/217] Fetching: ChIJp3rL1MWFyzsRRMEyhZl3SEo\n",
            "[72/217] Fetching: ChIJ36H6wH_J5zsRvRXnivhPlUM\n",
            "[73/217] Fetching: ChIJQxZ5QbNlXz4Ro92ZKfB7wp0\n",
            "[74/217] Fetching: ChIJ_VgAs2Z1SjoRLejSriWWOxE\n",
            "[75/217] Fetching: ChIJcbaFGTO_wjsRMZs7TfL0SCY\n",
            "[76/217] Fetching: ChIJ33DzZqRkGTkR3yca6-fGD5s\n",
            "[77/217] Fetching: ChIJE-l0UbrvDDkR_w2I1wt9fnc\n",
            "[78/217] Fetching: ChIJfYw9D34h4DsRqI5HQ1BvKjs\n",
            "[79/217] Fetching: ChIJQRRa5OwjDTkRGxAAyAgTB3Y\n",
            "[80/217] Fetching: ChIJEzyTCdNXmTkRKnvroCs4_OM\n",
            "[81/217] Fetching: ChIJIRC6w611AjoRyNBbBqrcvA4\n",
            "[82/217] Fetching: ChIJF--jlntrrjsRMuO_g_DGXsA\n",
            "[83/217] Fetching: ChIJqYVH2s4VrjsRwnVWd8GHd9E\n",
            "[84/217] Fetching: ChIJoQV1aBq55zsRNP51HJE9S0s\n",
            "[85/217] Fetching: ChIJk45YuGtDOToRVbSgKYBj2nU\n",
            "[86/217] Fetching: ChIJZU8k2vU9rjsRK9gXANkis7k\n",
            "[87/217] Fetching: ChIJ6XMgJgAPrjsR10XE1IqgLo8\n",
            "[88/217] Fetching: ChIJlRcbvehrXz4RYertkjZKHqU\n",
            "[89/217] Fetching: ChIJ____Pz3I5zsR71RaNnRLXEQ\n",
            "[90/217] Fetching: ChIJsxorwCUXrjsRaTXkIXLIjII\n",
            "[91/217] Fetching: ChIJK7aICxoVrjsRnjZbF0TjvNo\n",
            "[92/217] Fetching: ChIJfTOuhN04rTsRFHGJOnBGXQA\n",
            "[93/217] Fetching: ChIJZ2V_JlT7DDkR5fqS9GhJl5I\n",
            "[94/217] Fetching: ChIJBx5-uJ0iDTkRAo7D8XrBGdA\n",
            "[95/217] Fetching: ChIJrdAJ3F4f9zkRkIM0vrHrrlM\n",
            "[96/217] Fetching: ChIJNTDzhYOVwjsRb1PHKVhmz7c\n",
            "[97/217] Fetching: ChIJlaFtIZW75zsRuiYYCaRchdA\n",
            "[98/217] Fetching: ChIJo81VlUTwqzsRsO2vGK66o6E\n",
            "[99/217] Fetching: ChIJ0wtNa4qp5zsRNpiQVHESCrA\n",
            "[100/217] Fetching: ChIJy9wroGN1SjoRetZ4dnX4slc\n",
            "[101/217] Fetching: ChIJfXIpNTlhUjoRxUkcnjmtaI8\n",
            "[102/217] Fetching: ChIJcQ9AjHI3zDERUCrdCgq_QsY\n",
            "[103/217] Fetching: ChIJd3hIDSttrjsRzlYK59IhpnU\n",
            "[104/217] Fetching: ChIJoxm2LepmUjoR8RHioVGpdFY\n",
            "[105/217] Fetching: ChIJD1D-1hJ3AjoRE9MDbF2MNKs\n",
            "[106/217] Fetching: ChIJ4QwivsUDDTkRugm7OdDEnHY\n",
            "[107/217] Fetching: ChIJ59HkqBhvqTsR7ueGXwCrADA\n",
            "[108/217] Fetching: ChIJG4yuQZPA1DsRKxYcRB5Cjxo\n",
            "[109/217] Fetching: ChIJMdVYqQANCDsR9xXNA_CK9IY\n",
            "[110/217] Fetching: ChIJ6zp7bcbKmjkRBFUuR1FfrfI\n",
            "[111/217] Fetching: ChIJ4SS6j-CZyzsRJSltqhlhh2Y\n",
            "[112/217] Fetching: ChIJ7xgSLWMYrjsR3uZJjnDTyF0\n",
            "[113/217] Fetching: ChIJMbz4tfThDDkRGX0zEgCaJNQ\n",
            "[114/217] Fetching: ChIJ2Rg07Ah3AjoR8dFaCWZBvuQ\n",
            "[115/217] Fetching: ChIJx0L2Xb9kDDkRxhAS3pwRGWk\n",
            "[116/217] Fetching: ChIJX8QTFfCZ-DkRdIFpL0_JvYo\n",
            "[117/217] Fetching: ChIJ81VO7JEbDTkRFRWseAWl9QQ\n",
            "[118/217] Fetching: ChIJu-CoRKs_rjsRE-6uOfXfcOI\n",
            "[119/217] Fetching: ChIJr3uvYrF3AjoRadygoFqhiQg\n",
            "[120/217] Fetching: ChIJ3zECbkpFkTkRk2FpM_UA9XU\n",
            "[121/217] Fetching: ChIJ1YSnGRUNCDsRjVSpHO0z0GU\n",
            "[122/217] Fetching: ChIJcbg9R6tbGjkRZBDR_VtWPI8\n",
            "[123/217] Fetching: ChIJ5Woqc6wZrjsRllO5cUTnuxw\n",
            "[124/217] Fetching: ChIJ33PbSkvupzsRly-1-cmP-_A\n",
            "[125/217] Fetching: ChIJYSlT7R8ZDTkRTHOA7Kd3OpU\n",
            "[126/217] Fetching: ChIJ6TDQyDbdKDoR5jLxhAPrPAA\n",
            "[127/217] Fetching: ChIJCe7WfMxDfDkRKF98Ep1tIV8\n",
            "[128/217] Fetching: ChIJwf5BH97_kT4Rb3JOWC9I7s4\n",
            "[129/217] Fetching: ChIJ-VdpyJF7AjoR1Ud6WSO5KDI\n",
            "[130/217] Fetching: ChIJWWrjqCDCwjsRDNwKS-_xVMw\n",
            "[131/217] Fetching: ChIJPa7aabVbOToRRmNsMXq7Iw8\n",
            "[132/217] Fetching: ChIJsU0jC7UBDTkRNkemP9YevI8\n",
            "[133/217] Fetching: ChIJlQxDSBLI5zsRbosa-Dl-lLM\n",
            "[134/217] Fetching: ChIJHTN2JImRyzsR7PY5wu0fuVw\n",
            "[135/217] Fetching: ChIJq5euqyIbDTkRdJJZuIvoUk4\n",
            "[136/217] Fetching: ChIJkTwHcCt3AjoR-CfYm6089hk\n",
            "[137/217] Fetching: ChIJlXOFO0NY7TkRheDLAr2Chz8\n",
            "[138/217] Fetching: ChIJ0UU9Az3nDDkRbYmOXuOlGW4\n",
            "[139/217] Fetching: ChIJUwkQMVYDLz4RZu5bj7GWy74\n",
            "[140/217] Fetching: ChIJEZqisZOTyzsR4knixdB6pSo\n",
            "[141/217] Fetching: ChIJWUuyYuR3AjoRj78Bsvp_9P4\n",
            "[142/217] Fetching: ChIJOcFNQMvLWTkRKPPyxpEms6s\n",
            "[143/217] Fetching: ChIJ2R3KaI9zAjoR7qPbfF1nSFo\n",
            "[144/217] Fetching: ChIJezmM2NYDDTkRa2I0dNquuwA\n",
            "[145/217] Fetching: ChIJUftNROTD5zsROU7jeFo2N60\n",
            "[146/217] Fetching: ChIJE81GBI3I5zsR_WbE_9mCe4o\n",
            "[147/217] Fetching: ChIJufAK2mzI5zsR0gOEoEPuI4U\n",
            "[148/217] Fetching: ChIJB5Jt4-QRrjsRA8qETMLQX0Q\n",
            "[149/217] Fetching: ChIJH3VOGeX9DDkRMi8B5TLj7XY\n",
            "[150/217] Fetching: ChIJmaVf3D3J5zsRunDxXYOoqnM\n",
            "[151/217] Fetching: ChIJWUPXT6Y4nDkRJnXavNedSas\n",
            "[152/217] Fetching: ChIJ7SINfoHh9DkR9r_6v4zrIxc\n",
            "[153/217] Fetching: ChIJOc3MUF1nUjoRTWyZl11RHtM\n",
            "[154/217] Fetching: ChIJjebUezzjmzkRA1edAqZLj64\n",
            "[155/217] Fetching: ChIJy0njiDb9DDkR-9nuP9aWDc4\n",
            "[156/217] Fetching: ChIJdWr8EhLlDDkR5BwkTu4TDXg\n",
            "[157/217] Fetching: ChIJAwSByulN4DsR3K6NO-C2SCo\n",
            "[158/217] Fetching: ChIJL3a2pDKtgTkRwhNxltcZ2_E\n",
            "[159/217] Fetching: ChIJf1dhQIbO5zsR2Dk8FzQZyek\n",
            "[160/217] Fetching: ChIJWZ2DztCDGjkRo0p4e21mMng\n",
            "[161/217] Fetching: ChIJq6qq67FsXz4RAa6VcmbUvzw\n",
            "[162/217] Fetching: ChIJxcYrQLjnDDkR7rn-K4A86qE\n",
            "[163/217] Fetching: ChIJneH9IVANCDsRNJFSaCMFduA\n",
            "[164/217] Fetching: ChIJgUXOK7a1bTkR_VCxaPbA35w\n",
            "[165/217] Fetching: ChIJfc7qRCibXjkRbLy8ENeGRCA\n",
            "[166/217] Fetching: ChIJAysBFhKHXjkRIEQhSgKId1Y\n",
            "[167/217] Fetching: ChIJSd8n_P1cUjoRczF_vawgp7I\n",
            "[168/217] Fetching: ChIJdbNR7WS5wjsRUdR0TnhojN0\n",
            "[169/217] Fetching: ChIJ53QUdhDlmzkRiZ3TG_o7Mbs\n",
            "[170/217] Fetching: ChIJVVXBoh2e-DkRKEenw1n---I\n",
            "[171/217] Fetching: ChIJL8WffaukvDsR2yKT5zqJZ3g\n",
            "[172/217] Fetching: ChIJb93BuRp3AjoRqdxyBozdusM\n",
            "[173/217] Fetching: ChIJO7MEJBt3dDkRXPdZD57vt7s\n",
            "[174/217] Fetching: ChIJZdjp9ClzszsRlpWUebyU0Qw\n",
            "[175/217] Fetching: ChIJC1bb5kHo5zsRBGIhT1V4Fm8\n",
            "[176/217] Fetching: ChIJv58o7br6NToROPPDiCZ-HTU\n",
            "[177/217] Fetching: ChIJJ2suDYNxAjoRhQOuoXXwYA0\n",
            "[178/217] Fetching: ChIJmX2AiE4ZDTkRcTPFcLo6RIA\n",
            "[179/217] Fetching: ChIJk2FRchL9YjkRhSeLXrCucPQ\n",
            "[180/217] Fetching: ChIJD1N5jnij2zsRgz2InVbPAFc\n",
            "[181/217] Fetching: ChIJk788Eg31qjsR9ti7usE9z1Q\n",
            "[182/217] Fetching: ChIJV86eqPr9DDkRIyjiODcNXBM\n",
            "[183/217] Fetching: ChIJuQKYCq5XqDsRdmkL-92LmQo\n",
            "[184/217] Fetching: ChIJiWfiida9BTsRPwNqvbhCWXQ\n",
            "[185/217] Fetching: ChIJRTsTTwC99jkRDwCc_LCYPTc\n",
            "[186/217] Fetching: ChIJ3wP-ImIUrjsRnZ7Rr7BisQI\n",
            "[187/217] Fetching: ChIJncXNAhiXyzsRumni7kMunW0\n",
            "[188/217] Fetching: ChIJRSvhUjKFbzkRayLnV_VNAJM\n",
            "[189/217] Fetching: ChIJpXvXvijzTDoRcklrzv4Buxc\n",
            "[190/217] Fetching: ChIJBXvefWSbXjkR_oTsmrJI4LM\n",
            "[191/217] Fetching: ChIJA8rG94BhUzoRS_Hf2s1beqw\n",
            "[192/217] Fetching: ChIJ9XG3DJl9GTkR_Z1B2N77cRk\n",
            "[193/217] Fetching: ChIJjxcMKWu5wjsRfrvmKpqhVwY\n",
            "[194/217] Fetching: ChIJR_LjW7rFADsREtuUIiS18qA\n",
            "[195/217] Fetching: ChIJgxQS-3fAwjsRje6eRKPGAaM\n",
            "[196/217] Fetching: ChIJ7Zqp2oOTyzsRlbQ3sd9vXCc\n",
            "[197/217] Fetching: ChIJ25O3oqzD5zsR8MCLxAuyGwY\n",
            "[198/217] Fetching: ChIJ_Q6foItfUjoRCr1T1T9xr5o\n",
            "[199/217] Fetching: ChIJm7fENXTj9TkRN41A-HCQEQM\n",
            "[200/217] Fetching: ChIJy3fL91vkDDkRE82DgeVeJBo\n",
            "[201/217] Fetching: ChIJKUhqf73IXzkRox7AgorhYFI\n",
            "[202/217] Fetching: ChIJfTXgAFKTyzsR69kDPPdfQoQ\n",
            "[203/217] Fetching: ChIJNxHuVs-V5zsRNP07hNFbbYY\n",
            "[204/217] Fetching: ChIJNSur_WGZyzsRYIxoILXxcy0\n",
            "[205/217] Fetching: ChIJ82NsWaRZWjcRvSIoRmSg2h0\n",
            "[206/217] Fetching: ChIJqe-dMJoRrjsR7ZTs1d6lXMk\n",
            "[207/217] Fetching: ChIJ9_G0_64VrjsRHHqhcFPBiNY\n",
            "[208/217] Fetching: ChIJN7recGUTrjsR4yYqFyyo-AE\n",
            "[209/217] Fetching: ChIJs_k3s91NXjkRS0cUFxV9jBM\n",
            "[210/217] Fetching: ChIJG4bs3J15HDkRtPnxdsqVfE4\n",
            "[211/217] Fetching: ChIJ3QGUJHIXrjsRNesn2_KGLzc\n",
            "[212/217] Fetching: ChIJFT1JgIrqDDkRAffk6X2jmIk\n",
            "[213/217] Fetching: ChIJ27LB6fO35zsRLwxkS57dmEw\n",
            "[214/217] Fetching: ChIJLSw41N3P5zsRNUJ_e-7PgKQ\n",
            "[215/217] Fetching: ChIJdb-swxnBwjsROTq_vlbOEx8\n",
            "[216/217] Fetching: ChIJHdvcknDr3TsR04tcpO-kdig\n",
            "[217/217] Fetching: ChIJLQTnRyBX7TkRWhH8AdH99k8\n",
            "------------------------------\n",
            "Done. Added 1060 new reviews to /content/final_reviews_output.csv\n"
          ]
        }
      ]
    }
  ]
}